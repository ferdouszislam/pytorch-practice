{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcaiCkSm7e1yfCMDZlmkqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdouszislam/pytorch-practice/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12lgukzbI5h",
        "outputId": "1928d090-1f5e-45a8-d11e-728e9a1d8147"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU, yay!')\n",
        "else:\n",
        "  print('CPU :(')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgEIW4ARGzV"
      },
      "source": [
        "# Tutorial 02 - Tensor Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chx2s2PmOo7Y"
      },
      "source": [
        "x = torch.rand(3,3)\n",
        "y = torch.rand(3,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3mSyMQPXIC",
        "outputId": "092a8a0d-ccf7-4242-d4b4-8f7221bd8458"
      },
      "source": [
        "print(x, '\\n', y, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1462, 0.2233, 0.9211],\n",
            "        [0.3635, 0.7351, 0.5387],\n",
            "        [0.9280, 0.8394, 0.4823]]) \n",
            " tensor([[0.6323, 0.6516, 0.4581],\n",
            "        [0.8582, 0.2674, 0.8887],\n",
            "        [0.0805, 0.9711, 0.9377]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUrPReMQ2rY",
        "outputId": "b9daad20-a54a-49a1-efce-e72ca61d2f64"
      },
      "source": [
        "z = torch.add(x, y)\n",
        "z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7785, 0.8749, 1.3793],\n",
              "        [1.2217, 1.0025, 1.4274],\n",
              "        [1.0085, 1.8105, 1.4200]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uhX6x6QQsj",
        "outputId": "200b17d0-c326-4cbc-acd3-4b7dd8061942"
      },
      "source": [
        "z = torch.mul(x, y)\n",
        "z "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0924, 0.1455, 0.4220],\n",
              "        [0.3120, 0.1966, 0.4787],\n",
              "        [0.0747, 0.8152, 0.4523]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSZ4XYCY4Dp",
        "outputId": "6e999167-902d-4ce8-ff0a-acd4314e5309"
      },
      "source": [
        "z.add_(y) # same as z+=y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7248, 0.7971, 0.8801],\n",
              "        [1.1702, 0.4640, 1.3674],\n",
              "        [0.1553, 1.7863, 1.3900]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHW88RaR5ra",
        "outputId": "11bc5398-5fb0-4fb9-8aec-7cf429a4a782"
      },
      "source": [
        "z[:, 2] # get all rows at column 2 (0 based indexing)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8801, 1.3674, 1.3900])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNIPYZ-SJlD",
        "outputId": "89e3f96b-8f1f-4a59-9ece-b121f047e747"
      },
      "source": [
        "z[1,2].item() # get single element value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3674143552780151"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5EidfvTBAQ"
      },
      "source": [
        "'''\n",
        "input- a pytorch tensor variable \n",
        "returns- multiplication of the input tensor's dimensions \n",
        "'''\n",
        "def get_flat_shape(tensor):\n",
        "  dims=list(tensor.size())\n",
        "  flat_dim = 1\n",
        "  for dim in dims:\n",
        "    flat_dim*=dim\n",
        "  return flat_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7x_Wm3kUBEd",
        "outputId": "520316d1-8509-4a56-a8c9-85bcef8f8071"
      },
      "source": [
        "get_flat_shape(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Wk3k4YSW3G",
        "outputId": "115fce76-b2e8-4e21-f884-df01cad79ad1"
      },
      "source": [
        "flat_z = z.view(get_flat_shape(z)) # resizing a tensor\n",
        "flat_z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7248, 0.7971, 0.8801, 1.1702, 0.4640, 1.3674, 0.1553, 1.7863, 1.3900])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fB2PMBHWRHJ",
        "outputId": "b4e2c104-66eb-45ae-d153-5c1f7e1584aa"
      },
      "source": [
        "# tensor to numpy array conversion\n",
        "np_z = flat_z.clone().numpy() # using '.clone()' is a MUST\n",
        "np_z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72476834, 0.79708713, 0.88014144, 1.1702007 , 0.46396384,\n",
              "       1.3674144 , 0.15526175, 1.7862701 , 1.389991  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1MtPnKZyi5",
        "outputId": "a616cd0e-1181-4d75-acaf-b75982519079"
      },
      "source": [
        "# numpy array to tensor conversion\n",
        "flat_z = torch.from_numpy(np_z.copy()) # using '.copy()' is a MUST\n",
        "flat_z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7248, 0.7971, 0.8801, 1.1702, 0.4640, 1.3674, 0.1553, 1.7863, 1.3900])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CC9XZmKcP-_"
      },
      "source": [
        "  \n",
        "  **Tensors can be kept into GPU but numpy arrays have to remain on CPU. GPUs are generally faster.**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uD1SwRcgmt"
      },
      "source": [
        "device = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmS2CFUdBMh"
      },
      "source": [
        "# all operations on tensors to be done in GPU\n",
        "# x = torch.rand(2, 2).to(device)\n",
        "# y = torch.rand(2, 2).to(device)\n",
        "# z=x+y\n",
        "\n",
        "# print(x, '\\n', y, '\\n', z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFi3id9d7Kl",
        "outputId": "b396a860-a9dc-4c88-f309-d09544872ecf"
      },
      "source": [
        "# numpy arrays MUST be on cpu\n",
        "z = z.to('cpu')\n",
        "np_z = z.clone().numpy()\n",
        "np_z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.72476834, 0.79708713, 0.88014144],\n",
              "       [1.1702007 , 0.46396384, 1.3674144 ],\n",
              "       [0.15526175, 1.7862701 , 1.389991  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVKnRYzensC"
      },
      "source": [
        "# Tutorial 03 - Gradient Calculation with Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZDbHzXeuvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bae65b-3142-4e90-d186-7528099ef160"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.4671, -1.5343,  1.2387], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c1iYKHiVRm",
        "outputId": "c192b56a-0c57-4af8-9511-a2b222ad4651"
      },
      "source": [
        "y=x+2\n",
        "print(y)\n",
        "z=y*y*2\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.4671, 0.4657, 3.2387], grad_fn=<AddBackward0>)\n",
            "tensor([39.9097,  0.4338, 20.9777], grad_fn=<MulBackward0>)\n",
            "tensor(20.4404, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIr-7JvAnX7k",
        "outputId": "05863f8b-a291-4795-a8a9-433ed0b51b10"
      },
      "source": [
        "# calculating dz/dx for each element of x tensor (in this case- x1,x2,x3)\n",
        "#  N.B- All but the last call to backward should have the retain_graph=True option\n",
        "z.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.9561, 0.6209, 4.3182])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mCj276LGEC",
        "outputId": "be7ef010-6416-4ffb-a9c3-d4fff54fc070"
      },
      "source": [
        "# prevent gradient tracking \n",
        "# (might be needed when updating weights during training)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# way 1\n",
        "print('way 1')\n",
        "y=x\n",
        "y.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "# way 2\n",
        "print('way 2')\n",
        "y=x.detach()\n",
        "print(y)\n",
        "\n",
        "# way 3\n",
        "print('way 3')\n",
        "y=x+2\n",
        "print(x, '\\n', y)\n",
        "with torch.no_grad():\n",
        "  y=x+2\n",
        "  print(x, '\\n', y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.3228,  0.5156, -0.9595], requires_grad=True)\n",
            "way 1\n",
            "tensor([ 0.3228,  0.5156, -0.9595])\n",
            "way 2\n",
            "tensor([ 0.3228,  0.5156, -0.9595])\n",
            "way 3\n",
            "tensor([ 0.3228,  0.5156, -0.9595]) \n",
            " tensor([2.3228, 2.5156, 1.0405])\n",
            "tensor([ 0.3228,  0.5156, -0.9595]) \n",
            " tensor([2.3228, 2.5156, 1.0405])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsZzzjcWMlyQ",
        "outputId": "16d9a9d1-0cab-4704-951c-9ef0248eba1b"
      },
      "source": [
        "# dummy training example with some weights\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum() # loss function... probably\n",
        "  \n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  # before next iteration or optimization step MUST empty the gradient\n",
        "  weights.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJUSOKcaArS"
      },
      "source": [
        "# Tutorial 04 - Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI7Rq6laH1F",
        "outputId": "d60596f3-e461-42cc-9040-420ae0845895"
      },
      "source": [
        "# example backpropagation for a single instance\n",
        "\n",
        "x = torch.tensor(1.0) # input\n",
        "y = torch.tensor(2.0) # actual output\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # weight i.e, learnable parameter\n",
        "\n",
        "# forward pass\n",
        "y_hat = w*x # y_hat is the prediction using linear model = w*x \n",
        "loss = (y_hat - y)**2 # loss function = squared error (generally this would be MSE)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# now update weight using the gradient \n",
        "# and do forward and backward pass again"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWuYsc7Pkz5"
      },
      "source": [
        "# Tutorial 05 - Gradient Descent with Autograd & Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrrkb_gP1Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6791bb-6d2b-45dc-8c83-71ea71fdfd4b"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = np.array([1,2,3,4], dtype=np.float32) # input\n",
        "Y = np.array([2,4,6,8], dtype=np.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 5\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction, forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dJ_dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - learning_rate* dJ_dw \n",
        "\n",
        "  # print everytime\n",
        "  if epoch%1==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOYrO-OdpBP"
      },
      "source": [
        "### Now let's do the same using Autograd for backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWLzMGadnK1",
        "outputId": "3b86f397-e457-4406-ad33-bcf02dc79706"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # input\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "# def gradient(x, y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  #dJ_dw = gradient(X, Y, y_pred)\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights\n",
        "  #w = w - learning_rate* dJ_dw \n",
        "  w.data = w.data - learning_rate * w.grad\n",
        "  # alternately we can do this,\n",
        "  # with torch.no_grad(): \n",
        "  #   # update to weight should not be tracked for calculating gradient\n",
        "  #   w -= learning_rate*w.grad\n",
        "\n",
        "  w.grad.zero_() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OsGme4_5u_"
      },
      "source": [
        "# Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmGAvNn0C4pW"
      },
      "source": [
        "### Implementing Linear Regression same as before but this time with model, loss, optimizer, autograd from the **torch.nn** library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbeowTtbDg1"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNuXnTD_4c-",
        "outputId": "5eec80dd-f9ba-4509-f842-20a00551a549"
      },
      "source": [
        "'''\n",
        "Typical Training Pipeline\n",
        "\n",
        "1. design model (input size, output size, forward pass)\n",
        "2. Construct loss & optimizer\n",
        "3. Training loop\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: compute gradients\n",
        "  - update weights\n",
        "'''\n",
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) # input\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32) # actual output\n",
        "\n",
        "n_samples, n_features = X.shape # 4 samples each with one feature\n",
        "print('# of samples =', n_samples, ' # of features =', n_features, '\\n')\n",
        "\n",
        "# define custom model (same as Linear Regression for now)\n",
        "class MyLinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "# model = nn.Linear(in_features=n_features, out_features=1)\n",
        "model = MyLinearRegression(input_dim=n_features, output_dim=1)\n",
        "\n",
        "# test data\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "print(f'Prediction before training for x=5 : {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20 \n",
        "# MSE as loss function\n",
        "loss = nn.MSELoss()\n",
        "# optimize model with stochastic gradient descent\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, Y)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights using optimizer\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {J:.3f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {model(X_test).item():.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of samples = 4  # of features = 1 \n",
            "\n",
            "Prediction before training for x=5 : -2.978\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = -0.007, loss = 33.300\n",
            "epoch 4: w = 0.476, loss = 16.174\n",
            "epoch 6: w = 0.812, loss = 7.927\n",
            "epoch 8: w = 1.045, loss = 3.955\n",
            "epoch 10: w = 1.208, loss = 2.041\n",
            "epoch 12: w = 1.322, loss = 1.117\n",
            "epoch 14: w = 1.402, loss = 0.671\n",
            "epoch 16: w = 1.458, loss = 0.455\n",
            "epoch 18: w = 1.498, loss = 0.349\n",
            "epoch 20: w = 1.526, loss = 0.297\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 8.832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZg0E9-nahBo"
      },
      "source": [
        "# Tutorial 07 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJag9WMVanzP"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNoEQe9RbA7Y",
        "outputId": "c3964179-b799-45e4-a8ba-f8d40cc44eb9"
      },
      "source": [
        "# step-0 prepare data\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        " \n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0], 1) # convert y to row=n_samples and col=1\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# step-1 model\n",
        "model = nn.Linear(in_features=n_features, out_features=1)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.02\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, y)\n",
        "\n",
        "  # back prop\n",
        "  J.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  # clear grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch-{epoch+1}, loss = {J.item():.4f}')\n",
        "    for name, param in model.named_parameters():\n",
        "      print(name, '-', param)\n",
        "    print()\n",
        "\n",
        "# prediction\n",
        "y_pred = model(X).detach()\n",
        "\n",
        "# plot\n",
        "plt.plot(X_np, y_np, 'ro')\n",
        "plt.plot(X_np, y_pred.numpy(), 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch-10, loss = 1316.6125\n",
            "weight - Parameter containing:\n",
            "tensor([[13.2203]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.5404], requires_grad=True)\n",
            "\n",
            "epoch-20, loss = 828.6804\n",
            "weight - Parameter containing:\n",
            "tensor([[22.8059]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.5879], requires_grad=True)\n",
            "\n",
            "epoch-30, loss = 579.3090\n",
            "weight - Parameter containing:\n",
            "tensor([[29.6565]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.3346], requires_grad=True)\n",
            "\n",
            "epoch-40, loss = 451.0138\n",
            "weight - Parameter containing:\n",
            "tensor([[34.5611]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.0376], requires_grad=True)\n",
            "\n",
            "epoch-50, loss = 384.6354\n",
            "weight - Parameter containing:\n",
            "tensor([[38.0781]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.4310], requires_grad=True)\n",
            "\n",
            "epoch-60, loss = 350.1282\n",
            "weight - Parameter containing:\n",
            "tensor([[40.6039]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.7973], requires_grad=True)\n",
            "\n",
            "epoch-70, loss = 332.1183\n",
            "weight - Parameter containing:\n",
            "tensor([[42.4203]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.1162], requires_grad=True)\n",
            "\n",
            "epoch-80, loss = 322.6875\n",
            "weight - Parameter containing:\n",
            "tensor([[43.7282]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.3823], requires_grad=True)\n",
            "\n",
            "epoch-90, loss = 317.7359\n",
            "weight - Parameter containing:\n",
            "tensor([[44.6710]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.5983], requires_grad=True)\n",
            "\n",
            "epoch-100, loss = 315.1303\n",
            "weight - Parameter containing:\n",
            "tensor([[45.3514]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.7700], requires_grad=True)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0UhIqoXmJV6"
      },
      "source": [
        "# Tutorial 08 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZrC2wWofq-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9D8xt6ymIOu",
        "outputId": "b79c252d-aaf0-4de9-f4fd-a80ea75e7bd1"
      },
      "source": [
        "# step-0 prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# convert single row with multiple columns to multiple rows single colums\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "\n",
        "# step-1 model setup\n",
        "\n",
        "# logistic regression, f = w*x+b and apply sigmoid at the end\n",
        "class MyLogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(MyLogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(in_features=n_input_features, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = MyLogisticRegression(n_features)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.01\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # loss \n",
        "  J = loss(y_pred, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  J.backward()\n",
        "\n",
        "  # weight updates\n",
        "  optimizer.step()\n",
        "  # clear grads\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch {epoch+1}: loss = {J.item():.4f}')\n",
        "    # for name, param in model.named_parameters():\n",
        "    #   print(name, '-', param)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  cls_pred = y_pred.round() # if y_pred>=0.5 class = 1, else class = 0\n",
        "  test_acc = cls_pred.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'\\ntest accuracy = {test_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10: loss = 0.4891\n",
            "epoch 20: loss = 0.4360\n",
            "epoch 30: loss = 0.3969\n",
            "epoch 40: loss = 0.3668\n",
            "epoch 50: loss = 0.3426\n",
            "epoch 60: loss = 0.3228\n",
            "epoch 70: loss = 0.3062\n",
            "epoch 80: loss = 0.2919\n",
            "epoch 90: loss = 0.2796\n",
            "epoch 100: loss = 0.2687\n",
            "\n",
            "test accuracy = 59.702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hkn9nXMH2j"
      },
      "source": [
        "# Tutorial 9 Dataset and DataLoader - Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL9hwn7_MQDe"
      },
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "bLvhesmrikgb",
        "outputId": "fd2389ed-4f75-4ff9-eec6-81c08e09dfbc"
      },
      "source": [
        "'''\n",
        "epoch = one forward and backward pass of ALL training samples\n",
        "batch_size = number of training samples used in one forward/backward pass\n",
        "number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\n",
        "e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nepoch = one forward and backward pass of ALL training samples\\nbatch_size = number of training samples used in one forward/backward pass\\nnumber of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\\ne.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaP-PD7FesRr",
        "outputId": "c7e1b360-e5c6-4e4f-a066-b8689319bc08"
      },
      "source": [
        "# mount gdrive with this code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbUxpCm9Z-Ca"
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(wine_ds[:, 1:]) # take all rows akip first column\n",
        "    self.y = torch.from_numpy(wine_ds[:, [0]]) # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8XkFSKyaMVP",
        "outputId": "dea4f016-71d5-469b-c9d2-5a414c1e321d"
      },
      "source": [
        "wine_ds = WineDataset()\n",
        "wine_ds[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
              "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
              "         1.0650e+03]), tensor([1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98KixUYIhPyf",
        "outputId": "14ce468f-311f-4d18-b6d4-50a1039d71b5"
      },
      "source": [
        "dataloader = DataLoader(dataset=wine_ds, batch_size=4, shuffle=True, num_workers=2)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "data = data_iter.next()\n",
        "features, labels = data\n",
        "print(features, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.1460e+01, 3.7400e+00, 1.8200e+00, 1.9500e+01, 1.0700e+02, 3.1800e+00,\n",
            "         2.5800e+00, 2.4000e-01, 3.5800e+00, 2.9000e+00, 7.5000e-01, 2.8100e+00,\n",
            "         5.6200e+02],\n",
            "        [1.1660e+01, 1.8800e+00, 1.9200e+00, 1.6000e+01, 9.7000e+01, 1.6100e+00,\n",
            "         1.5700e+00, 3.4000e-01, 1.1500e+00, 3.8000e+00, 1.2300e+00, 2.1400e+00,\n",
            "         4.2800e+02],\n",
            "        [1.4060e+01, 1.6300e+00, 2.2800e+00, 1.6000e+01, 1.2600e+02, 3.0000e+00,\n",
            "         3.1700e+00, 2.4000e-01, 2.1000e+00, 5.6500e+00, 1.0900e+00, 3.7100e+00,\n",
            "         7.8000e+02],\n",
            "        [1.2850e+01, 1.6000e+00, 2.5200e+00, 1.7800e+01, 9.5000e+01, 2.4800e+00,\n",
            "         2.3700e+00, 2.6000e-01, 1.4600e+00, 3.9300e+00, 1.0900e+00, 3.6300e+00,\n",
            "         1.0150e+03]]) tensor([[2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTX8Ptpki0XP",
        "outputId": "1edfd951-d656-4ec9-be63-a40d1f64dde7"
      },
      "source": [
        "# iterate over dataset by batched (dummy train loop)\n",
        "num_epochs = 2\n",
        "total_sample = len(wine_ds)\n",
        "num_iterations = math.ceil(total_sample/4)\n",
        "print('total samples =', total_sample, '\\n# of iterations(per epoch) =', num_iterations, '\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for step, (inputs, labels) in enumerate(dataloader):\n",
        "    if (step+1)%5==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {step+1}/{num_iterations}, input_dim - {inputs.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total samples = 178 \n",
            "# of iterations(per epoch) = 45 \n",
            "\n",
            "epoch 1/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, input_dim - torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, input_dim - torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aky_2rAm7E8"
      },
      "source": [
        "# Tutorial 10 Dataset Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9o9kYUPv492"
      },
      "source": [
        "'''\n",
        "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
        "during creation of the DataSet\n",
        "\n",
        "complete list of built-in transforms: \n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "On Images\n",
        "---------\n",
        "CenterCrop, Grayscale, Pad, RandomAffine\n",
        "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
        "Resize, Scale\n",
        "\n",
        "On Tensors\n",
        "----------\n",
        "LinearTransformation, Normalize, RandomErasing\n",
        "\n",
        "Conversion\n",
        "----------\n",
        "ToPILImage: from tensor or ndrarray\n",
        "ToTensor : from numpy.ndarray or PILImage\n",
        "\n",
        "Generic\n",
        "-------\n",
        "Use Lambda \n",
        "\n",
        "Custom\n",
        "------\n",
        "Write own class\n",
        "\n",
        "Compose multiple Transforms\n",
        "---------------------------\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elw0nDLBkMze"
      },
      "source": [
        "# dataset class with transform\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, transform=None):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = wine_ds[:, 1:] # take all rows akip first column\n",
        "    self.y = wine_ds[:, [0]] # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEH8dcyHsUfA"
      },
      "source": [
        "# custom transform classes (transform using callable objects)\n",
        "\n",
        "class MyToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MyMulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs*=self.factor\n",
        "    return inputs, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RhxQ2ltFKv",
        "outputId": "8225367f-3af3-4f23-edaf-93d09dd911fd"
      },
      "source": [
        "dataset = WineDataset()\n",
        "features, labels = dataset[0]\n",
        "print('without transform:', type(features), type(labels))\n",
        "print('without transform: features =', features)\n",
        "\n",
        "dataset = WineDataset(transform=MyToTensor())\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyToTensor() transform:', type(features), type(labels))\n",
        "\n",
        "dataset = WineDataset(transform=MyMulTransform(2))\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyMulTransform(2) transform: features =', features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "without transform: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "without transform: features = [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
            "\n",
            "with MyToTensor() transform: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "\n",
            "with MyMulTransform(2) transform: features = [2.846e+01 3.420e+00 4.860e+00 3.120e+01 2.540e+02 5.600e+00 6.120e+00\n",
            " 5.600e-01 4.580e+00 1.128e+01 2.080e+00 7.840e+00 2.130e+03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enO2D5rgjxsX"
      },
      "source": [
        "# Tutorial 11 Softmax and Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYUN0W4WkBrk",
        "outputId": "4ae1fa27-0e1e-4772-bd4d-aa833c6214e8"
      },
      "source": [
        "# softmax using numpy\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "sm_x = softmax(x)\n",
        "print(sm_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.65900114 0.24243297 0.09856589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2Mi4fXndhw",
        "outputId": "85033372-8e60-494a-86cf-4bf5c21c387d"
      },
      "source": [
        "# softmax using pytorch\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "sm_x = torch.softmax(x, dim=0)\n",
        "print(sm_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Q7XPRJtdJ7",
        "outputId": "f62b39ed-7b8e-4dc5-c11e-ccc44eb93a73"
      },
      "source": [
        "# cross entropy using numpy\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "  loss = -np.sum(actual * np.log(predicted))\n",
        "  return loss # / float(predicted.shape[0])\n",
        "\n",
        "y_actual = np.array([1, 0, 0]) # one hot encoded\n",
        "\n",
        "y_good_pred = np.array([0.7, 0.2, 0.1])\n",
        "y_bad_pred = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(y_actual, y_good_pred)\n",
        "l2 = cross_entropy(y_actual, y_bad_pred)\n",
        "print(f'loss for good prediction={l1:.3f}')\n",
        "print(f'loss for bad prediction={l2:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss for good prediction=0.357\n",
            "loss for bad prediction=2.303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJT7jjhfvMJs",
        "outputId": "401096a3-48a0-4587-8a8e-00a133c00a55"
      },
      "source": [
        "# cross entropy using pytorch (with multiple samples)\n",
        "\n",
        "'''\n",
        "nn.CrossEntropyLoss() automatically applies, \n",
        "  nn.LogSoftmax + nn.NLLLoss(neg log likelihood loss)\n",
        "\n",
        "- Don't apply softmax in the last layer yourself\n",
        "- y_actual has class labels, not One Hot Encoded!\n",
        "- y_pred has raw scores and not prob values after softmax\n",
        "'''\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "y_actual = torch.tensor([2, 0, 1]) # 3 samples\n",
        "\n",
        "# 3 sample each sample can be one of 3 classes\n",
        "y_pred_good = torch.tensor([ [0.2, 1.0, 2.0],[2.0, 1.0, 0.1],[1.0, 2.0, 0.1] ])\n",
        "y_pred_bad = torch.tensor([ [0.5, 2.0, 0.3],[0.5, 1.0, 2.3],[7.5, 2.0, 1.3] ]) \n",
        "\n",
        "l1 = loss(y_pred_good, y_actual)\n",
        "l2 = loss(y_pred_bad, y_actual)\n",
        "print('good pred loss =', l1.item())\n",
        "print('bad pred loss =', l2.item())\n",
        "\n",
        "_, class_pred_good = torch.max(y_pred_good, 1)\n",
        "_, class_pred_bad = torch.max(y_pred_bad, 1)\n",
        "print('\\ngood class prediction =', class_pred_good)\n",
        "print('bad class prediction =', class_pred_bad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good pred loss = 0.4204676151275635\n",
            "bad pred loss = 3.236616373062134\n",
            "\n",
            "good class prediction = tensor([2, 0, 1])\n",
            "bad class prediction = tensor([1, 2, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xEe5i8E80rb"
      },
      "source": [
        "### Implementing Basic Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqfaCRFc0TVs"
      },
      "source": [
        "# custom multi-class neural network with one hidden layer and relu activation\n",
        "class MultiClassNeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(MultiClassNeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # input layer to hidden layer\n",
        "    output = self.relu(self.linear(input))\n",
        "    # hidden layer to output layer\n",
        "    output = self.linear2(output)\n",
        "    \n",
        "    # don't do softmax because nn.CrossEntropy does it for us\n",
        "\n",
        "    return output\n",
        "\n",
        "model = MultiClassNeuralNet(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "loss = nn.CrossEntropyLoss() # applies softmax to the output automatically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGGOyRZQ_Dbm"
      },
      "source": [
        "# custom binary neural network with one hidden layer and relu activation\n",
        "class BinaryClassNeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(BinaryClassNeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # input layer to hidden layer\n",
        "    output = self.relu(self.linear1(input))\n",
        "    # hidden layer to output layer\n",
        "    output = self.linear2(output)\n",
        "    # probability prediction\n",
        "    y_pred = torch.sigmoid(output)\n",
        "    return y_pred\n",
        "\n",
        "model = BinaryClassNeuralNet(input_size=28*28, hidden_size=5)\n",
        "loss = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}