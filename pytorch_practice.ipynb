{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4PVfMdYd8HT1Vf719si6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdouszislam/pytorch-practice/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12lgukzbI5h",
        "outputId": "4af08129-c6c2-4e93-e060-9b4b3a24f248"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU, yay!')\n",
        "else:\n",
        "  print('CPU :(')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgEIW4ARGzV"
      },
      "source": [
        "# Tutorial 02 - Tensor Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chx2s2PmOo7Y"
      },
      "source": [
        "x = torch.rand(3,3)\n",
        "y = torch.rand(3,3)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3mSyMQPXIC",
        "outputId": "ce97740e-afa9-4777-bc69-060b51c0cb85"
      },
      "source": [
        "print(x, '\\n', y, '\\n')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.3991, 0.9063, 0.1599],\n",
            "        [0.3815, 0.6069, 0.2662],\n",
            "        [0.0536, 0.3494, 0.9179]]) \n",
            " tensor([[0.1000, 0.3890, 0.5162],\n",
            "        [0.3167, 0.8101, 0.8929],\n",
            "        [0.3259, 0.7029, 0.5643]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUrPReMQ2rY",
        "outputId": "c0681444-8366-422f-e15a-bae673c91858"
      },
      "source": [
        "z = torch.add(x, y)\n",
        "z"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4991, 1.2952, 0.6761],\n",
              "        [0.6982, 1.4170, 1.1591],\n",
              "        [0.3794, 1.0523, 1.4823]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uhX6x6QQsj",
        "outputId": "658acd4c-61d0-4e2c-feaf-d250f65a7d8b"
      },
      "source": [
        "z = torch.mul(x, y)\n",
        "z "
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0399, 0.3525, 0.0826],\n",
              "        [0.1208, 0.4917, 0.2377],\n",
              "        [0.0175, 0.2456, 0.5180]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSZ4XYCY4Dp",
        "outputId": "4a503bf0-a39a-4b6d-ef7f-87037bf1ef23"
      },
      "source": [
        "z.add_(y) # same as z+=y"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1399, 0.7415, 0.5987],\n",
              "        [0.4375, 1.3018, 1.1306],\n",
              "        [0.3433, 0.9485, 1.0824]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHW88RaR5ra",
        "outputId": "63fdc846-5759-431a-a1b7-6a6622efef8e"
      },
      "source": [
        "z[:, 2] # get all rows at column 2 (0 based indexing)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5987, 1.1306, 1.0824])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNIPYZ-SJlD",
        "outputId": "c86c81bd-35ba-4d43-c77e-c79ff5fbde58"
      },
      "source": [
        "z[1,2].item() # get single element value"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.130584955215454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5EidfvTBAQ"
      },
      "source": [
        "'''\n",
        "input- a pytorch tensor variable \n",
        "returns- multiplication of the input tensor's dimensions \n",
        "'''\n",
        "def get_flat_shape(tensor):\n",
        "  dims=list(tensor.size())\n",
        "  flat_dim = 1\n",
        "  for dim in dims:\n",
        "    flat_dim*=dim\n",
        "  return flat_dim"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7x_Wm3kUBEd",
        "outputId": "929b1271-26d8-476c-ac2d-5825f068d079"
      },
      "source": [
        "get_flat_shape(z)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Wk3k4YSW3G",
        "outputId": "a9b58329-730e-41ce-d61f-9150ae75c924"
      },
      "source": [
        "flat_z = z.view(get_flat_shape(z)) # resizing a tensor\n",
        "flat_z"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1399, 0.7415, 0.5987, 0.4375, 1.3018, 1.1306, 0.3433, 0.9485, 1.0824])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fB2PMBHWRHJ",
        "outputId": "5d31f900-a1d7-4cc3-bea8-324665192994"
      },
      "source": [
        "# tensor to numpy array conversion\n",
        "np_z = flat_z.clone().numpy() # using '.clone()' is a MUST\n",
        "np_z"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.13993661, 0.74147767, 0.59871256, 0.4374787 , 1.3017585 ,\n",
              "       1.130585  , 0.3433221 , 0.94846123, 1.0823648 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1MtPnKZyi5",
        "outputId": "36d009eb-918b-4011-bfcf-d5316d06475b"
      },
      "source": [
        "# numpy array to tensor conversion\n",
        "flat_z = torch.from_numpy(np_z.copy()) # using '.copy()' is a MUST\n",
        "flat_z"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1399, 0.7415, 0.5987, 0.4375, 1.3018, 1.1306, 0.3433, 0.9485, 1.0824])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CC9XZmKcP-_"
      },
      "source": [
        "  \n",
        "  **Tensors can be kept into GPU but numpy arrays have to remain on CPU. GPUs are generally faster.**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uD1SwRcgmt"
      },
      "source": [
        "device = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmS2CFUdBMh"
      },
      "source": [
        "# all operations on tensors to be done in GPU\n",
        "# x = torch.rand(2, 2).to(device)\n",
        "# y = torch.rand(2, 2).to(device)\n",
        "# z=x+y\n",
        "\n",
        "# print(x, '\\n', y, '\\n', z)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFi3id9d7Kl",
        "outputId": "0b2b1b3a-25ab-4d4d-b0f8-f94df7d4eb11"
      },
      "source": [
        "# numpy arrays MUST be on cpu\n",
        "z = z.to('cpu')\n",
        "np_z = z.clone().numpy()\n",
        "np_z"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13993661, 0.74147767, 0.59871256],\n",
              "       [0.4374787 , 1.3017585 , 1.130585  ],\n",
              "       [0.3433221 , 0.94846123, 1.0823648 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVKnRYzensC"
      },
      "source": [
        "# Tutorial 03 - Gradient Calculation with Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZDbHzXeuvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f711e1-ef0d-4e92-9c43-dc710033dde6"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "x"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9298, -0.0608,  0.3859], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c1iYKHiVRm",
        "outputId": "f9d916cd-c5fa-4550-fcba-81e2201b99ae"
      },
      "source": [
        "y=x+2\n",
        "print(y)\n",
        "z=y*y*2\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z) "
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0702, 1.9392, 2.3859], grad_fn=<AddBackward0>)\n",
            "tensor([ 2.2908,  7.5208, 11.3847], grad_fn=<MulBackward0>)\n",
            "tensor(7.0654, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIr-7JvAnX7k",
        "outputId": "f8fc2948-915b-46b8-e601-8faea963b680"
      },
      "source": [
        "# calculating dz/dx for each element of x tensor (in this case- x1,x2,x3)\n",
        "#  N.B- All but the last call to backward should have the retain_graph=True option\n",
        "z.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.4270, 2.5856, 3.1812])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mCj276LGEC",
        "outputId": "bc487ada-77dc-4371-aa38-c983385e079a"
      },
      "source": [
        "# prevent gradient tracking \n",
        "# (might be needed when updating weights during training)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# way 1\n",
        "print('way 1')\n",
        "y=x\n",
        "y.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "# way 2\n",
        "print('way 2')\n",
        "y=x.detach()\n",
        "print(y)\n",
        "\n",
        "# way 3\n",
        "print('way 3')\n",
        "y=x+2\n",
        "print(x, '\\n', y)\n",
        "with torch.no_grad():\n",
        "  y=x+2\n",
        "  print(x, '\\n', y)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.5241,  0.7123,  2.3696], requires_grad=True)\n",
            "way 1\n",
            "tensor([-1.5241,  0.7123,  2.3696])\n",
            "way 2\n",
            "tensor([-1.5241,  0.7123,  2.3696])\n",
            "way 3\n",
            "tensor([-1.5241,  0.7123,  2.3696]) \n",
            " tensor([0.4759, 2.7123, 4.3696])\n",
            "tensor([-1.5241,  0.7123,  2.3696]) \n",
            " tensor([0.4759, 2.7123, 4.3696])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsZzzjcWMlyQ",
        "outputId": "1a8cc8e2-7e1a-4d52-ab8f-987b768d2663"
      },
      "source": [
        "# dummy training example with some weights\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum() # loss function... probably\n",
        "  \n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  # before next iteration or optimization step MUST empty the gradient\n",
        "  weights.grad.zero_()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJUSOKcaArS"
      },
      "source": [
        "# Tutorial 04 - Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI7Rq6laH1F",
        "outputId": "9c440035-23bf-4f1b-d3c4-073ae89bd0f0"
      },
      "source": [
        "# example backpropagation for a single instance\n",
        "\n",
        "x = torch.tensor(1.0) # input\n",
        "y = torch.tensor(2.0) # actual output\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # weight i.e, learnable parameter\n",
        "\n",
        "# forward pass\n",
        "y_hat = w*x # y_hat is the prediction using linear model = w*x \n",
        "loss = (y_hat - y)**2 # loss function = squared error (generally this would be MSE)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# now update weight using the gradient \n",
        "# and do forward and backward pass again"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWuYsc7Pkz5"
      },
      "source": [
        "# Tutorial 05 - Gradient Descent with Autograd & Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrrkb_gP1Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f6cc66-2c42-4c6f-ff29-4261f6b6ac55"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = np.array([1,2,3,4], dtype=np.float32) # input\n",
        "Y = np.array([2,4,6,8], dtype=np.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 5\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction, forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dJ_dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - learning_rate* dJ_dw \n",
        "\n",
        "  # print everytime\n",
        "  if epoch%1==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOYrO-OdpBP"
      },
      "source": [
        "### Now let's do the same using Autograd for backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWLzMGadnK1",
        "outputId": "1a83fe46-aab3-4fe5-b635-72d8986e08dd"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # input\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "# def gradient(x, y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  #dJ_dw = gradient(X, Y, y_pred)\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights\n",
        "  #w = w - learning_rate* dJ_dw \n",
        "  w.data = w.data - learning_rate * w.grad\n",
        "  # alternately we can do this,\n",
        "  # with torch.no_grad(): \n",
        "  #   # update to weight should not be tracked for calculating gradient\n",
        "  #   w -= learning_rate*w.grad\n",
        "\n",
        "  w.grad.zero_() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OsGme4_5u_"
      },
      "source": [
        "# Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmGAvNn0C4pW"
      },
      "source": [
        "### Implementing Linear Regression same as before but this time with model, loss, optimizer, autograd from the **torch.nn** library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbeowTtbDg1"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNuXnTD_4c-",
        "outputId": "35cb3d86-5e40-4bc4-90cf-2a938da81440"
      },
      "source": [
        "'''\n",
        "Typical Training Pipeline\n",
        "\n",
        "1. design model (input size, output size, forward pass)\n",
        "2. Construct loss & optimizer\n",
        "3. Training loop\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: compute gradients\n",
        "  - update weights\n",
        "'''\n",
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) # input\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32) # actual output\n",
        "\n",
        "n_samples, n_features = X.shape # 4 samples each with one feature\n",
        "print('# of samples =', n_samples, ' # of features =', n_features, '\\n')\n",
        "\n",
        "# define custom model (same as Linear Regression for now)\n",
        "class MyLinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "# model = nn.Linear(in_features=n_features, out_features=1)\n",
        "model = MyLinearRegression(input_dim=n_features, output_dim=1)\n",
        "\n",
        "# test data\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "print(f'Prediction before training for x=5 : {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20 \n",
        "# MSE as loss function\n",
        "loss = nn.MSELoss()\n",
        "# optimize model with stochastic gradient descent\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, Y)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights using optimizer\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {J:.3f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {model(X_test).item():.3f}')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of samples = 4  # of features = 1 \n",
            "\n",
            "Prediction before training for x=5 : 1.249\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.641, loss = 15.085\n",
            "epoch 4: w = 0.966, loss = 7.331\n",
            "epoch 6: w = 1.192, loss = 3.597\n",
            "epoch 8: w = 1.349, loss = 1.798\n",
            "epoch 10: w = 1.459, loss = 0.931\n",
            "epoch 12: w = 1.536, loss = 0.513\n",
            "epoch 14: w = 1.589, loss = 0.311\n",
            "epoch 16: w = 1.627, loss = 0.213\n",
            "epoch 18: w = 1.654, loss = 0.165\n",
            "epoch 20: w = 1.673, loss = 0.141\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZg0E9-nahBo"
      },
      "source": [
        "# Tutorial 07 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJag9WMVanzP"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNoEQe9RbA7Y",
        "outputId": "a04e69a3-dbc2-4e05-9d51-0f16ac750845"
      },
      "source": [
        "# step-0 prepare data\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        " \n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0], 1) # convert y to row=n_samples and col=1\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# step-1 model\n",
        "model = nn.Linear(in_features=n_features, out_features=1)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.02\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, y)\n",
        "\n",
        "  # back prop\n",
        "  J.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  # clear grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch-{epoch+1}, loss = {J.item():.4f}')\n",
        "    for name, param in model.named_parameters():\n",
        "      print(name, '-', param)\n",
        "    print()\n",
        "\n",
        "# prediction\n",
        "y_pred = model(X).detach()\n",
        "\n",
        "# plot\n",
        "plt.plot(X_np, y_np, 'ro')\n",
        "plt.plot(X_np, y_pred.numpy(), 'b')\n",
        "plt.show()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch-10, loss = 1302.9143\n",
            "weight - Parameter containing:\n",
            "tensor([[13.4384]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-1.2556], requires_grad=True)\n",
            "\n",
            "epoch-20, loss = 823.4680\n",
            "weight - Parameter containing:\n",
            "tensor([[22.9405]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-1.0573], requires_grad=True)\n",
            "\n",
            "epoch-30, loss = 577.4164\n",
            "weight - Parameter containing:\n",
            "tensor([[29.7388]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.6429], requires_grad=True)\n",
            "\n",
            "epoch-40, loss = 450.3802\n",
            "weight - Parameter containing:\n",
            "tensor([[34.6108]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.1651], requires_grad=True)\n",
            "\n",
            "epoch-50, loss = 384.4566\n",
            "weight - Parameter containing:\n",
            "tensor([[38.1076]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.2976], requires_grad=True)\n",
            "\n",
            "epoch-60, loss = 350.1002\n",
            "weight - Parameter containing:\n",
            "tensor([[40.6210]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.7094], requires_grad=True)\n",
            "\n",
            "epoch-70, loss = 332.1318\n",
            "weight - Parameter containing:\n",
            "tensor([[42.4299]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.0582], requires_grad=True)\n",
            "\n",
            "epoch-80, loss = 322.7068\n",
            "weight - Parameter containing:\n",
            "tensor([[43.7333]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.3440], requires_grad=True)\n",
            "\n",
            "epoch-90, loss = 317.7513\n",
            "weight - Parameter containing:\n",
            "tensor([[44.6736]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.5729], requires_grad=True)\n",
            "\n",
            "epoch-100, loss = 315.1406\n",
            "weight - Parameter containing:\n",
            "tensor([[45.3525]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.7532], requires_grad=True)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0UhIqoXmJV6"
      },
      "source": [
        "# Tutorial 08 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZrC2wWofq-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9D8xt6ymIOu",
        "outputId": "07b4c37f-3386-4cdc-901e-8485f8d24276"
      },
      "source": [
        "# step-0 prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# convert single row with multiple columns to multiple rows single colums\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "\n",
        "# step-1 model setup\n",
        "\n",
        "# logistic regression, f = w*x+b and apply sigmoid at the end\n",
        "class MyLogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(MyLogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(in_features=n_input_features, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = MyLogisticRegression(n_features)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.01\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # loss \n",
        "  J = loss(y_pred, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  J.backward()\n",
        "\n",
        "  # weight updates\n",
        "  optimizer.step()\n",
        "  # clear grads\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch {epoch+1}: loss = {J.item():.4f}')\n",
        "    # for name, param in model.named_parameters():\n",
        "    #   print(name, '-', param)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  cls_pred = y_pred.round() # if y_pred>=0.5 class = 1, else class = 0\n",
        "  test_acc = cls_pred.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'\\ntest accuracy = {test_acc:.3f}')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10: loss = 0.8524\n",
            "epoch 20: loss = 0.6360\n",
            "epoch 30: loss = 0.5150\n",
            "epoch 40: loss = 0.4407\n",
            "epoch 50: loss = 0.3908\n",
            "epoch 60: loss = 0.3548\n",
            "epoch 70: loss = 0.3275\n",
            "epoch 80: loss = 0.3061\n",
            "epoch 90: loss = 0.2887\n",
            "epoch 100: loss = 0.2743\n",
            "\n",
            "test accuracy = 60.439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hkn9nXMH2j"
      },
      "source": [
        "# Tutorial 9 Dataset and DataLoader - Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL9hwn7_MQDe"
      },
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "bLvhesmrikgb",
        "outputId": "d60080c2-e70e-482c-d53e-3a6f41803ef2"
      },
      "source": [
        "'''\n",
        "epoch = one forward and backward pass of ALL training samples\n",
        "batch_size = number of training samples used in one forward/backward pass\n",
        "number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\n",
        "e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
        "'''"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nepoch = one forward and backward pass of ALL training samples\\nbatch_size = number of training samples used in one forward/backward pass\\nnumber of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\\ne.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaP-PD7FesRr",
        "outputId": "50127ce8-ad84-401d-bef1-e14a98dea47a"
      },
      "source": [
        "# mount gdrive with this code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbUxpCm9Z-Ca"
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(wine_ds[:, 1:]) # take all rows akip first column\n",
        "    self.y = torch.from_numpy(wine_ds[:, [0]]) # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "    "
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8XkFSKyaMVP",
        "outputId": "b947c131-adef-406c-8e54-8fd05b172165"
      },
      "source": [
        "wine_ds = WineDataset()\n",
        "wine_ds[0]"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
              "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
              "         1.0650e+03]), tensor([1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98KixUYIhPyf",
        "outputId": "632a936a-0167-478e-c9ce-a443e24648d7"
      },
      "source": [
        "dataloader = DataLoader(dataset=wine_ds, batch_size=4, shuffle=True, num_workers=2)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "data = data_iter.next()\n",
        "features, labels = data\n",
        "print(features, labels)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.2510e+01, 1.2400e+00, 2.2500e+00, 1.7500e+01, 8.5000e+01, 2.0000e+00,\n",
            "         5.8000e-01, 6.0000e-01, 1.2500e+00, 5.4500e+00, 7.5000e-01, 1.5100e+00,\n",
            "         6.5000e+02],\n",
            "        [1.4370e+01, 1.9500e+00, 2.5000e+00, 1.6800e+01, 1.1300e+02, 3.8500e+00,\n",
            "         3.4900e+00, 2.4000e-01, 2.1800e+00, 7.8000e+00, 8.6000e-01, 3.4500e+00,\n",
            "         1.4800e+03],\n",
            "        [1.2290e+01, 2.8300e+00, 2.2200e+00, 1.8000e+01, 8.8000e+01, 2.4500e+00,\n",
            "         2.2500e+00, 2.5000e-01, 1.9900e+00, 2.1500e+00, 1.1500e+00, 3.3000e+00,\n",
            "         2.9000e+02],\n",
            "        [1.2370e+01, 1.1700e+00, 1.9200e+00, 1.9600e+01, 7.8000e+01, 2.1100e+00,\n",
            "         2.0000e+00, 2.7000e-01, 1.0400e+00, 4.6800e+00, 1.1200e+00, 3.4800e+00,\n",
            "         5.1000e+02]]) tensor([[3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTX8Ptpki0XP",
        "outputId": "7f0fde6e-27d5-490a-e936-24e5c30fa500"
      },
      "source": [
        "# iterate over dataset by batched (dummy train loop)\n",
        "num_epochs = 2\n",
        "total_sample = len(wine_ds)\n",
        "num_iterations = math.ceil(total_sample/4)\n",
        "print('total samples =', total_sample, '\\n# of iterations(per epoch) =', num_iterations, '\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for step, (inputs, labels) in enumerate(dataloader):\n",
        "    if (step+1)%5==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {step+1}/{num_iterations}, input_dim - {inputs.shape}')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total samples = 178 \n",
            "# of iterations(per epoch) = 45 \n",
            "\n",
            "epoch 1/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, input_dim - torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, input_dim - torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aky_2rAm7E8"
      },
      "source": [
        "# Tutorial 10 Dataset Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9o9kYUPv492"
      },
      "source": [
        "'''\n",
        "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
        "during creation of the DataSet\n",
        "\n",
        "complete list of built-in transforms: \n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "On Images\n",
        "---------\n",
        "CenterCrop, Grayscale, Pad, RandomAffine\n",
        "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
        "Resize, Scale\n",
        "\n",
        "On Tensors\n",
        "----------\n",
        "LinearTransformation, Normalize, RandomErasing\n",
        "\n",
        "Conversion\n",
        "----------\n",
        "ToPILImage: from tensor or ndrarray\n",
        "ToTensor : from numpy.ndarray or PILImage\n",
        "\n",
        "Generic\n",
        "-------\n",
        "Use Lambda \n",
        "\n",
        "Custom\n",
        "------\n",
        "Write own class\n",
        "\n",
        "Compose multiple Transforms\n",
        "---------------------------\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elw0nDLBkMze"
      },
      "source": [
        "# dataset class with transform\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, transform=None):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = wine_ds[:, 1:] # take all rows akip first column\n",
        "    self.y = wine_ds[:, [0]] # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEH8dcyHsUfA"
      },
      "source": [
        "# custom transform classes (transform using callable objects)\n",
        "\n",
        "class MyToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MyMulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs*=self.factor\n",
        "    return inputs, target"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RhxQ2ltFKv",
        "outputId": "1a09e697-812c-4f77-ad35-ebe38ed3d0a8"
      },
      "source": [
        "dataset = WineDataset()\n",
        "features, labels = dataset[0]\n",
        "print('without transform:', type(features), type(labels))\n",
        "print('without transform: features =', features)\n",
        "\n",
        "dataset = WineDataset(transform=MyToTensor())\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyToTensor() transform:', type(features), type(labels))\n",
        "\n",
        "dataset = WineDataset(transform=MyMulTransform(2))\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyMulTransform(2) transform: features =', features)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "without transform: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "without transform: features = [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
            "\n",
            "with MyToTensor() transform: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "\n",
            "with MyMulTransform(2) transform: features = [2.846e+01 3.420e+00 4.860e+00 3.120e+01 2.540e+02 5.600e+00 6.120e+00\n",
            " 5.600e-01 4.580e+00 1.128e+01 2.080e+00 7.840e+00 2.130e+03]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}