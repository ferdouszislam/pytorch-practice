{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSs8zDkGcjMJjHBWINizWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdouszislam/pytorch-practice/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12lgukzbI5h",
        "outputId": "6195e451-cd45-4a33-f158-99a8e67c5ebc"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU, yay!')\n",
        "else:\n",
        "  print('CPU :(')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgEIW4ARGzV"
      },
      "source": [
        "# Tutorial 02 - Tensor Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chx2s2PmOo7Y"
      },
      "source": [
        "x = torch.rand(3,3)\n",
        "y = torch.rand(3,3)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3mSyMQPXIC",
        "outputId": "d7fe8332-911a-453c-eec5-5fd1701b9b53"
      },
      "source": [
        "print(x, '\\n', y, '\\n')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2429, 0.4869, 0.6601],\n",
            "        [0.4873, 0.6931, 0.3529],\n",
            "        [0.1914, 0.5219, 0.5159]]) \n",
            " tensor([[0.7332, 0.0484, 0.8020],\n",
            "        [0.9021, 0.8039, 0.4301],\n",
            "        [0.5350, 0.0076, 0.7921]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUrPReMQ2rY",
        "outputId": "5c45b4ab-581c-446c-ab5b-790791e93237"
      },
      "source": [
        "z = torch.add(x, y)\n",
        "z"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9762, 0.5353, 1.4621],\n",
              "        [1.3894, 1.4970, 0.7830],\n",
              "        [0.7264, 0.5295, 1.3080]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uhX6x6QQsj",
        "outputId": "6544e952-322d-49c2-c3aa-54c6c7c1f873"
      },
      "source": [
        "z = torch.mul(x, y)\n",
        "z "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1781, 0.0236, 0.5294],\n",
              "        [0.4396, 0.5572, 0.1518],\n",
              "        [0.1024, 0.0040, 0.4086]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSZ4XYCY4Dp",
        "outputId": "2aa2d24d-10b7-4906-81aa-7addd0067c69"
      },
      "source": [
        "z.add_(y) # same as z+=y"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9114, 0.0720, 1.3314],\n",
              "        [1.3418, 1.3611, 0.5819],\n",
              "        [0.6374, 0.0116, 1.2007]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHW88RaR5ra",
        "outputId": "12ce4840-a04e-4574-a143-4095bbeecd8c"
      },
      "source": [
        "z[:, 2] # get all rows at column 2 (0 based indexing)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3314, 0.5819, 1.2007])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNIPYZ-SJlD",
        "outputId": "63fbfd6a-ad60-435f-eea0-d2d45943639b"
      },
      "source": [
        "z[1,2].item() # get single element value"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.581874430179596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5EidfvTBAQ"
      },
      "source": [
        "'''\n",
        "input- a pytorch tensor variable \n",
        "returns- multiplication of the input tensor's dimensions \n",
        "'''\n",
        "def get_flat_shape(tensor):\n",
        "  dims=list(tensor.size())\n",
        "  flat_dim = 1\n",
        "  for dim in dims:\n",
        "    flat_dim*=dim\n",
        "  return flat_dim"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7x_Wm3kUBEd",
        "outputId": "deb769ff-c082-4bb7-c30a-353d39832b01"
      },
      "source": [
        "get_flat_shape(z)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Wk3k4YSW3G",
        "outputId": "d03b895f-619f-4dc2-86f9-dd3319019793"
      },
      "source": [
        "flat_z = z.view(get_flat_shape(z)) # resizing a tensor\n",
        "flat_z"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9114, 0.0720, 1.3314, 1.3418, 1.3611, 0.5819, 0.6374, 0.0116, 1.2007])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fB2PMBHWRHJ",
        "outputId": "5e4ee5f3-69eb-4d6f-95ef-e0169a9598b9"
      },
      "source": [
        "# tensor to numpy array conversion\n",
        "np_z = flat_z.clone().numpy() # using '.clone()' is a MUST\n",
        "np_z"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91138047, 0.07199942, 1.3314435 , 1.3417524 , 1.3611152 ,\n",
              "       0.58187443, 0.6374462 , 0.01155822, 1.2006778 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1MtPnKZyi5",
        "outputId": "b896375f-0868-41a0-cf44-ff427ea24795"
      },
      "source": [
        "# numpy array to tensor conversion\n",
        "flat_z = torch.from_numpy(np_z.copy()) # using '.copy()' is a MUST\n",
        "flat_z"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9114, 0.0720, 1.3314, 1.3418, 1.3611, 0.5819, 0.6374, 0.0116, 1.2007])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CC9XZmKcP-_"
      },
      "source": [
        "  \n",
        "  **Tensors can be kept into GPU but numpy arrays have to remain on CPU. GPUs are generally faster.**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uD1SwRcgmt"
      },
      "source": [
        "device = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmS2CFUdBMh"
      },
      "source": [
        "# all operations on tensors to be done in GPU\n",
        "# x = torch.rand(2, 2).to(device)\n",
        "# y = torch.rand(2, 2).to(device)\n",
        "# z=x+y\n",
        "\n",
        "# print(x, '\\n', y, '\\n', z)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFi3id9d7Kl",
        "outputId": "aafb12bd-31f8-4542-9e2a-68d0c3264170"
      },
      "source": [
        "# numpy arrays MUST be on cpu\n",
        "z = z.to('cpu')\n",
        "np_z = z.clone().numpy()\n",
        "np_z"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91138047, 0.07199942, 1.3314435 ],\n",
              "       [1.3417524 , 1.3611152 , 0.58187443],\n",
              "       [0.6374462 , 0.01155822, 1.2006778 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVKnRYzensC"
      },
      "source": [
        "# Tutorial 03 - Gradient Calculation with Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZDbHzXeuvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7063758-5d7f-4383-ead2-190621777bde"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "x"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4545,  1.2116, -2.0397], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c1iYKHiVRm",
        "outputId": "9ba882dd-f27c-4cf0-e701-5b5273230dd8"
      },
      "source": [
        "y=x+2\n",
        "print(y)\n",
        "z=y*y*2\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z) "
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.5455,  3.2116, -0.0397], grad_fn=<AddBackward0>)\n",
            "tensor([4.7774e+00, 2.0629e+01, 3.1444e-03], grad_fn=<MulBackward0>)\n",
            "tensor(8.4698, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIr-7JvAnX7k",
        "outputId": "0b163bc7-f617-47bb-8a69-d7cdf4550adb"
      },
      "source": [
        "# calculating dz/dx for each element of x tensor (in this case- x1,x2,x3)\n",
        "#  N.B- All but the last call to backward should have the retain_graph=True option\n",
        "z.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.0607,  4.2821, -0.0529])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mCj276LGEC",
        "outputId": "7dc1026c-fbe2-46a5-c6b5-fcf735b04a06"
      },
      "source": [
        "# prevent gradient tracking \n",
        "# (might be needed when updating weights during training)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# way 1\n",
        "print('way 1')\n",
        "y=x\n",
        "y.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "# way 2\n",
        "print('way 2')\n",
        "y=x.detach()\n",
        "print(y)\n",
        "\n",
        "# way 3\n",
        "print('way 3')\n",
        "y=x+2\n",
        "print(x, '\\n', y)\n",
        "with torch.no_grad():\n",
        "  y=x+2\n",
        "  print(x, '\\n', y)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.3457, -0.2724,  1.1655], requires_grad=True)\n",
            "way 1\n",
            "tensor([ 0.3457, -0.2724,  1.1655])\n",
            "way 2\n",
            "tensor([ 0.3457, -0.2724,  1.1655])\n",
            "way 3\n",
            "tensor([ 0.3457, -0.2724,  1.1655]) \n",
            " tensor([2.3457, 1.7276, 3.1655])\n",
            "tensor([ 0.3457, -0.2724,  1.1655]) \n",
            " tensor([2.3457, 1.7276, 3.1655])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsZzzjcWMlyQ",
        "outputId": "8b958500-90fe-4f77-cc55-0a404841ae94"
      },
      "source": [
        "# dummy training example with some weights\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum() # loss function... probably\n",
        "  \n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  # before next iteration or optimization step MUST empty the gradient\n",
        "  weights.grad.zero_()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJUSOKcaArS"
      },
      "source": [
        "# Tutorial 04 - Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI7Rq6laH1F",
        "outputId": "a4ac15f6-c5dc-4358-97d5-98a90bf4cc71"
      },
      "source": [
        "# example backpropagation for a single instance\n",
        "\n",
        "x = torch.tensor(1.0) # input\n",
        "y = torch.tensor(2.0) # actual output\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # weight i.e, learnable parameter\n",
        "\n",
        "# forward pass\n",
        "y_hat = w*x # y_hat is the prediction using linear model = w*x \n",
        "loss = (y_hat - y)**2 # loss function = squared error (generally this would be MSE)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# now update weight using the gradient \n",
        "# and do forward and backward pass again"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWuYsc7Pkz5"
      },
      "source": [
        "# Tutorial 05 - Gradient Descent with Autograd & Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrrkb_gP1Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4ba828-9f52-4976-fcda-a5d5c3415618"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = np.array([1,2,3,4], dtype=np.float32) # input\n",
        "Y = np.array([2,4,6,8], dtype=np.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 5\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction, forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dJ_dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - learning_rate* dJ_dw \n",
        "\n",
        "  # print everytime\n",
        "  if epoch%1==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOYrO-OdpBP"
      },
      "source": [
        "### Now let's do the same using Autograd for backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWLzMGadnK1",
        "outputId": "f04ded80-ddbc-412f-f8c4-066b6a7c8065"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # input\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "# def gradient(x, y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  #dJ_dw = gradient(X, Y, y_pred)\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights\n",
        "  #w = w - learning_rate* dJ_dw \n",
        "  w.data = w.data - learning_rate * w.grad\n",
        "  # alternately we can do this,\n",
        "  # with torch.no_grad(): \n",
        "  #   # update to weight should not be tracked for calculating gradient\n",
        "  #   w -= learning_rate*w.grad\n",
        "\n",
        "  w.grad.zero_() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OsGme4_5u_"
      },
      "source": [
        "# Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmGAvNn0C4pW"
      },
      "source": [
        "### Implementing Linear Regression same as before but this time with model, loss, optimizer, autograd from the **torch.nn** library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbeowTtbDg1"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNuXnTD_4c-",
        "outputId": "0673a245-42e4-4c74-94c2-993411646278"
      },
      "source": [
        "'''\n",
        "Typical Training Pipeline\n",
        "\n",
        "1. design model (input size, output size, forward pass)\n",
        "2. Construct loss & optimizer\n",
        "3. Training loop\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: compute gradients\n",
        "  - update weights\n",
        "'''\n",
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) # input\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32) # actual output\n",
        "\n",
        "n_samples, n_features = X.shape # 4 samples each with one feature\n",
        "print('# of samples =', n_samples, ' # of features =', n_features, '\\n')\n",
        "\n",
        "# define custom model (same as Linear Regression for now)\n",
        "class MyLinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "# model = nn.Linear(in_features=n_features, out_features=1)\n",
        "model = MyLinearRegression(input_dim=n_features, output_dim=1)\n",
        "\n",
        "# test data\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "print(f'Prediction before training for x=5 : {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20 \n",
        "# MSE as loss function\n",
        "loss = nn.MSELoss()\n",
        "# optimize model with stochastic gradient descent\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, Y)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights using optimizer\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {J:.3f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {model(X_test).item():.3f}')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of samples = 4  # of features = 1 \n",
            "\n",
            "Prediction before training for x=5 : 2.243\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 1.038, loss = 14.105\n",
            "epoch 4: w = 1.351, loss = 6.794\n",
            "epoch 6: w = 1.568, loss = 3.274\n",
            "epoch 8: w = 1.719, loss = 1.579\n",
            "epoch 10: w = 1.823, loss = 0.763\n",
            "epoch 12: w = 1.896, loss = 0.370\n",
            "epoch 14: w = 1.946, loss = 0.181\n",
            "epoch 16: w = 1.980, loss = 0.090\n",
            "epoch 18: w = 2.004, loss = 0.046\n",
            "epoch 20: w = 2.021, loss = 0.025\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZg0E9-nahBo"
      },
      "source": [
        "# Tutorial 07 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJag9WMVanzP"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNoEQe9RbA7Y",
        "outputId": "6b694c70-ab93-4d3f-edbc-d0fe62303fbb"
      },
      "source": [
        "# step-0 prepare data\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        " \n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0], 1) # convert y to row=n_samples and col=1\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# step-1 model\n",
        "model = nn.Linear(in_features=n_features, out_features=1)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.02\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, y)\n",
        "\n",
        "  # back prop\n",
        "  J.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  # clear grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch-{epoch+1}, loss = {J.item():.4f}')\n",
        "    for name, param in model.named_parameters():\n",
        "      print(name, '-', param)\n",
        "    print()\n",
        "\n",
        "# prediction\n",
        "y_pred = model(X).detach()\n",
        "\n",
        "# plot\n",
        "plt.plot(X_np, y_np, 'ro')\n",
        "plt.plot(X_np, y_pred.numpy(), 'b')\n",
        "plt.show()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch-10, loss = 1271.0977\n",
            "weight - Parameter containing:\n",
            "tensor([[14.0005]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.3902], requires_grad=True)\n",
            "\n",
            "epoch-20, loss = 805.0642\n",
            "weight - Parameter containing:\n",
            "tensor([[23.3680]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.4648], requires_grad=True)\n",
            "\n",
            "epoch-30, loss = 567.0027\n",
            "weight - Parameter containing:\n",
            "tensor([[30.0620]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.2359], requires_grad=True)\n",
            "\n",
            "epoch-40, loss = 444.5781\n",
            "weight - Parameter containing:\n",
            "tensor([[34.8539]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.1153], requires_grad=True)\n",
            "\n",
            "epoch-50, loss = 381.2599\n",
            "weight - Parameter containing:\n",
            "tensor([[38.2897]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.4914], requires_grad=True)\n",
            "\n",
            "epoch-60, loss = 348.3536\n",
            "weight - Parameter containing:\n",
            "tensor([[40.7570]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.8438], requires_grad=True)\n",
            "\n",
            "epoch-70, loss = 331.1834\n",
            "weight - Parameter containing:\n",
            "tensor([[42.5311]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.1516], requires_grad=True)\n",
            "\n",
            "epoch-80, loss = 322.1943\n",
            "weight - Parameter containing:\n",
            "tensor([[43.8084]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.4092], requires_grad=True)\n",
            "\n",
            "epoch-90, loss = 317.4753\n",
            "weight - Parameter containing:\n",
            "tensor([[44.7292]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.6186], requires_grad=True)\n",
            "\n",
            "epoch-100, loss = 314.9925\n",
            "weight - Parameter containing:\n",
            "tensor([[45.3936]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.7852], requires_grad=True)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0UhIqoXmJV6"
      },
      "source": [
        "# Tutorial 08 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZrC2wWofq-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9D8xt6ymIOu",
        "outputId": "18289a42-a491-4776-dbd4-98ef67edc956"
      },
      "source": [
        "# step-0 prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# convert single row with multiple columns to multiple rows single colums\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "\n",
        "# step-1 model setup\n",
        "\n",
        "# logistic regression, f = w*x+b and apply sigmoid at the end\n",
        "class MyLogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(MyLogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(in_features=n_input_features, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = MyLogisticRegression(n_features)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.01\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # loss \n",
        "  J = loss(y_pred, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  J.backward()\n",
        "\n",
        "  # weight updates\n",
        "  optimizer.step()\n",
        "  # clear grads\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch {epoch+1}: loss = {J.item():.4f}')\n",
        "    # for name, param in model.named_parameters():\n",
        "    #   print(name, '-', param)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  cls_pred = y_pred.round() # if y_pred>=0.5 class = 1, else class = 0\n",
        "  test_acc = cls_pred.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'\\ntest accuracy = {test_acc:.3f}')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10: loss = 0.4691\n",
            "epoch 20: loss = 0.4023\n",
            "epoch 30: loss = 0.3574\n",
            "epoch 40: loss = 0.3251\n",
            "epoch 50: loss = 0.3006\n",
            "epoch 60: loss = 0.2813\n",
            "epoch 70: loss = 0.2656\n",
            "epoch 80: loss = 0.2526\n",
            "epoch 90: loss = 0.2415\n",
            "epoch 100: loss = 0.2321\n",
            "\n",
            "test accuracy = 60.439\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}