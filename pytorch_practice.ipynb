{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFAxLOdkmIiwVLf7OWlSAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdouszislam/pytorch-practice/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12lgukzbI5h",
        "outputId": "6f742932-1eed-47e6-8896-052d0a64875f"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU, yay!')\n",
        "else:\n",
        "  print('CPU :(')"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgEIW4ARGzV"
      },
      "source": [
        "# Tutorial 02 - Tensor Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chx2s2PmOo7Y"
      },
      "source": [
        "x = torch.rand(3,3)\n",
        "y = torch.rand(3,3)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3mSyMQPXIC",
        "outputId": "93b94c53-a35f-4443-e9ef-b5ba0fef6027"
      },
      "source": [
        "print(x, '\\n', y, '\\n')"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1837, 0.2798, 0.2174],\n",
            "        [0.0621, 0.8123, 0.9890],\n",
            "        [0.9186, 0.8131, 0.7220]]) \n",
            " tensor([[0.3029, 0.0965, 0.2047],\n",
            "        [0.7264, 0.6094, 0.5638],\n",
            "        [0.1965, 0.2648, 0.2891]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUrPReMQ2rY",
        "outputId": "0021d1ae-454d-4466-ec72-0b7d76533e21"
      },
      "source": [
        "z = torch.add(x, y)\n",
        "z"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4865, 0.3763, 0.4221],\n",
              "        [0.7885, 1.4217, 1.5529],\n",
              "        [1.1151, 1.0779, 1.0111]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uhX6x6QQsj",
        "outputId": "0990a95a-ec55-40b0-a811-54ec7c1f489f"
      },
      "source": [
        "z = torch.mul(x, y)\n",
        "z "
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0556, 0.0270, 0.0445],\n",
              "        [0.0451, 0.4950, 0.5576],\n",
              "        [0.1805, 0.2153, 0.2088]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSZ4XYCY4Dp",
        "outputId": "58284693-8aaf-4e41-8950-0cb7357046fa"
      },
      "source": [
        "z.add_(y) # same as z+=y"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3585, 0.1235, 0.2492],\n",
              "        [0.7715, 1.1044, 1.1215],\n",
              "        [0.3771, 0.4800, 0.4979]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHW88RaR5ra",
        "outputId": "91aecc47-6e1a-4288-b4b4-b732b6b17ae0"
      },
      "source": [
        "z[:, 2] # get all rows at column 2 (0 based indexing)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2492, 1.1215, 0.4979])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNIPYZ-SJlD",
        "outputId": "f0156fbc-51c3-443b-baa9-162983c26cd2"
      },
      "source": [
        "z[1,2].item() # get single element value"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.121473789215088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5EidfvTBAQ"
      },
      "source": [
        "'''\n",
        "input- a pytorch tensor variable \n",
        "returns- multiplication of the input tensor's dimensions \n",
        "'''\n",
        "def get_flat_shape(tensor):\n",
        "  dims=list(tensor.size())\n",
        "  flat_dim = 1\n",
        "  for dim in dims:\n",
        "    flat_dim*=dim\n",
        "  return flat_dim"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7x_Wm3kUBEd",
        "outputId": "6efb495e-2034-497f-dd8f-5029fec5eb31"
      },
      "source": [
        "get_flat_shape(z)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Wk3k4YSW3G",
        "outputId": "c5442c15-6d94-4f02-8355-9e119611ac18"
      },
      "source": [
        "flat_z = z.view(get_flat_shape(z)) # resizing a tensor\n",
        "flat_z"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3585, 0.1235, 0.2492, 0.7715, 1.1044, 1.1215, 0.3771, 0.4800, 0.4979])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fB2PMBHWRHJ",
        "outputId": "c4cb3c97-cef8-4081-a39b-6ac254cdd6ec"
      },
      "source": [
        "# tensor to numpy array conversion\n",
        "np_z = flat_z.clone().numpy() # using '.clone()' is a MUST\n",
        "np_z"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.35850933, 0.12352349, 0.24919325, 0.771486  , 1.1043884 ,\n",
              "       1.1214738 , 0.37709093, 0.4800402 , 0.49788365], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1MtPnKZyi5",
        "outputId": "95799b0a-3910-4a6c-d400-2f79646626f9"
      },
      "source": [
        "# numpy array to tensor conversion\n",
        "flat_z = torch.from_numpy(np_z.copy()) # using '.copy()' is a MUST\n",
        "flat_z"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3585, 0.1235, 0.2492, 0.7715, 1.1044, 1.1215, 0.3771, 0.4800, 0.4979])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CC9XZmKcP-_"
      },
      "source": [
        "  \n",
        "  **Tensors can be kept into GPU but numpy arrays have to remain on CPU. GPUs are generally faster.**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uD1SwRcgmt"
      },
      "source": [
        "device = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmS2CFUdBMh"
      },
      "source": [
        "# all operations on tensors to be done in GPU\n",
        "# x = torch.rand(2, 2).to(device)\n",
        "# y = torch.rand(2, 2).to(device)\n",
        "# z=x+y\n",
        "\n",
        "# print(x, '\\n', y, '\\n', z)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFi3id9d7Kl",
        "outputId": "26f05322-2e87-4f2d-a3b1-d3e8c94e93aa"
      },
      "source": [
        "# numpy arrays MUST be on cpu\n",
        "z = z.to('cpu')\n",
        "np_z = z.clone().numpy()\n",
        "np_z"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35850933, 0.12352349, 0.24919325],\n",
              "       [0.771486  , 1.1043884 , 1.1214738 ],\n",
              "       [0.37709093, 0.4800402 , 0.49788365]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVKnRYzensC"
      },
      "source": [
        "# Tutorial 03 - Gradient Calculation with Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZDbHzXeuvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9007a3-0ee2-433c-fde8-d7949596bb48"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "x"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0475,  0.0511, -2.6235], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c1iYKHiVRm",
        "outputId": "2054be12-e3e4-4617-e0d9-aab6ca8afc75"
      },
      "source": [
        "y=x+2\n",
        "print(y)\n",
        "z=y*y*2\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z) "
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.9525,  2.0511, -0.6235], grad_fn=<AddBackward0>)\n",
            "tensor([1.8146, 8.4136, 0.7774], grad_fn=<MulBackward0>)\n",
            "tensor(3.6685, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIr-7JvAnX7k",
        "outputId": "92175f21-7e46-431f-94c9-08649d5ebaf8"
      },
      "source": [
        "# calculating dz/dx for each element of x tensor (in this case- x1,x2,x3)\n",
        "#  N.B- All but the last call to backward should have the retain_graph=True option\n",
        "z.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.2700,  2.7347, -0.8313])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mCj276LGEC",
        "outputId": "9f4a65ca-0adc-4599-9080-82e33ed9af41"
      },
      "source": [
        "# prevent gradient tracking \n",
        "# (might be needed when updating weights during training)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# way 1\n",
        "print('way 1')\n",
        "y=x\n",
        "y.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "# way 2\n",
        "print('way 2')\n",
        "y=x.detach()\n",
        "print(y)\n",
        "\n",
        "# way 3\n",
        "print('way 3')\n",
        "y=x+2\n",
        "print(x, '\\n', y)\n",
        "with torch.no_grad():\n",
        "  y=x+2\n",
        "  print(x, '\\n', y)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.7938, -0.1984, -1.5807], requires_grad=True)\n",
            "way 1\n",
            "tensor([-0.7938, -0.1984, -1.5807])\n",
            "way 2\n",
            "tensor([-0.7938, -0.1984, -1.5807])\n",
            "way 3\n",
            "tensor([-0.7938, -0.1984, -1.5807]) \n",
            " tensor([1.2062, 1.8016, 0.4193])\n",
            "tensor([-0.7938, -0.1984, -1.5807]) \n",
            " tensor([1.2062, 1.8016, 0.4193])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsZzzjcWMlyQ",
        "outputId": "9d4e5696-19ab-4021-b30a-98497303fa73"
      },
      "source": [
        "# dummy training example with some weights\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum() # loss function... probably\n",
        "  \n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  # before next iteration or optimization step MUST empty the gradient\n",
        "  weights.grad.zero_()"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJUSOKcaArS"
      },
      "source": [
        "# Tutorial 04 - Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI7Rq6laH1F",
        "outputId": "d952bcd0-4a8e-46cb-ab71-79a853b38bf2"
      },
      "source": [
        "# example backpropagation for a single instance\n",
        "\n",
        "x = torch.tensor(1.0) # input\n",
        "y = torch.tensor(2.0) # actual output\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # weight i.e, learnable parameter\n",
        "\n",
        "# forward pass\n",
        "y_hat = w*x # y_hat is the prediction using linear model = w*x \n",
        "loss = (y_hat - y)**2 # loss function = squared error (generally this would be MSE)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# now update weight using the gradient \n",
        "# and do forward and backward pass again"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWuYsc7Pkz5"
      },
      "source": [
        "# Tutorial 05 - Gradient Descent with Autograd & Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrrkb_gP1Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6070d510-6794-49d7-a03b-cb057b03fb4a"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = np.array([1,2,3,4], dtype=np.float32) # input\n",
        "Y = np.array([2,4,6,8], dtype=np.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 5\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction, forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dJ_dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - learning_rate* dJ_dw \n",
        "\n",
        "  # print everytime\n",
        "  if epoch%1==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOYrO-OdpBP"
      },
      "source": [
        "### Now let's do the same using Autograd for backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWLzMGadnK1",
        "outputId": "4999b2fc-3f20-4e20-ab48-3520d8b35cdb"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # input\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "# def gradient(x, y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  #dJ_dw = gradient(X, Y, y_pred)\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights\n",
        "  #w = w - learning_rate* dJ_dw \n",
        "  w.data = w.data - learning_rate * w.grad\n",
        "  # alternately we can do this,\n",
        "  # with torch.no_grad(): \n",
        "  #   # update to weight should not be tracked for calculating gradient\n",
        "  #   w -= learning_rate*w.grad\n",
        "\n",
        "  w.grad.zero_() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OsGme4_5u_"
      },
      "source": [
        "# Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmGAvNn0C4pW"
      },
      "source": [
        "### Implementing Linear Regression same as before but this time with model, loss, optimizer, autograd from the **torch.nn** library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbeowTtbDg1"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNuXnTD_4c-",
        "outputId": "c757cb2b-d672-4314-a131-8f2d505f824f"
      },
      "source": [
        "'''\n",
        "Typical Training Pipeline\n",
        "\n",
        "1. design model (input size, output size, forward pass)\n",
        "2. Construct loss & optimizer\n",
        "3. Training loop\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: compute gradients\n",
        "  - update weights\n",
        "'''\n",
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) # input\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32) # actual output\n",
        "\n",
        "n_samples, n_features = X.shape # 4 samples each with one feature\n",
        "print('# of samples =', n_samples, ' # of features =', n_features, '\\n')\n",
        "\n",
        "# define custom model (same as Linear Regression for now)\n",
        "class MyLinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "# model = nn.Linear(in_features=n_features, out_features=1)\n",
        "model = MyLinearRegression(input_dim=n_features, output_dim=1)\n",
        "\n",
        "# test data\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "print(f'Prediction before training for x=5 : {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20 \n",
        "# MSE as loss function\n",
        "loss = nn.MSELoss()\n",
        "# optimize model with stochastic gradient descent\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, Y)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights using optimizer\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {J:.3f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {model(X_test).item():.3f}')"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of samples = 4  # of features = 1 \n",
            "\n",
            "Prediction before training for x=5 : -1.119\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.431, loss = 26.324\n",
            "epoch 4: w = 0.860, loss = 12.695\n",
            "epoch 6: w = 1.159, loss = 6.133\n",
            "epoch 8: w = 1.366, loss = 2.974\n",
            "epoch 10: w = 1.510, loss = 1.452\n",
            "epoch 12: w = 1.610, loss = 0.719\n",
            "epoch 14: w = 1.680, loss = 0.366\n",
            "epoch 16: w = 1.729, loss = 0.196\n",
            "epoch 18: w = 1.763, loss = 0.114\n",
            "epoch 20: w = 1.787, loss = 0.074\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZg0E9-nahBo"
      },
      "source": [
        "# Tutorial 07 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJag9WMVanzP"
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNoEQe9RbA7Y",
        "outputId": "50260809-1e59-45ad-bb63-2a8e293c5087"
      },
      "source": [
        "# step-0 prepare data\n",
        "X_np, y_np = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
        " \n",
        "X = torch.from_numpy(X_np.astype(np.float32))\n",
        "y = torch.from_numpy(y_np.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0], 1) # convert y to row=n_samples and col=1\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# step-1 model\n",
        "model = nn.Linear(in_features=n_features, out_features=1)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.02\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(y_pred, y)\n",
        "\n",
        "  # back prop\n",
        "  J.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  # clear grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch-{epoch+1}, loss = {J.item():.4f}')\n",
        "    for name, param in model.named_parameters():\n",
        "      print(name, '-', param)\n",
        "    print()\n",
        "\n",
        "# prediction\n",
        "y_pred = model(X).detach()\n",
        "\n",
        "# plot\n",
        "plt.plot(X_np, y_np, 'ro')\n",
        "plt.plot(X_np, y_pred.numpy(), 'b')\n",
        "plt.show()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch-10, loss = 1286.7487\n",
            "weight - Parameter containing:\n",
            "tensor([[13.7147]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-1.1800], requires_grad=True)\n",
            "\n",
            "epoch-20, loss = 815.0140\n",
            "weight - Parameter containing:\n",
            "tensor([[23.1403]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.9988], requires_grad=True)\n",
            "\n",
            "epoch-30, loss = 572.9822\n",
            "weight - Parameter containing:\n",
            "tensor([[29.8834]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.5980], requires_grad=True)\n",
            "\n",
            "epoch-40, loss = 448.0487\n",
            "weight - Parameter containing:\n",
            "tensor([[34.7154]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([-0.1310], requires_grad=True)\n",
            "\n",
            "epoch-50, loss = 383.2282\n",
            "weight - Parameter containing:\n",
            "tensor([[38.1834]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.3234], requires_grad=True)\n",
            "\n",
            "epoch-60, loss = 349.4521\n",
            "weight - Parameter containing:\n",
            "tensor([[40.6760]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([0.7289], requires_grad=True)\n",
            "\n",
            "epoch-70, loss = 331.7894\n",
            "weight - Parameter containing:\n",
            "tensor([[42.4698]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.0728], requires_grad=True)\n",
            "\n",
            "epoch-80, loss = 322.5257\n",
            "weight - Parameter containing:\n",
            "tensor([[43.7623]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.3549], requires_grad=True)\n",
            "\n",
            "epoch-90, loss = 317.6554\n",
            "weight - Parameter containing:\n",
            "tensor([[44.6946]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.5810], requires_grad=True)\n",
            "\n",
            "epoch-100, loss = 315.0898\n",
            "weight - Parameter containing:\n",
            "tensor([[45.3677]], requires_grad=True)\n",
            "bias - Parameter containing:\n",
            "tensor([1.7592], requires_grad=True)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0UhIqoXmJV6"
      },
      "source": [
        "# Tutorial 08 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBZrC2wWofq-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEMz56NHuhNW"
      },
      "source": [
        "# torch.manual_seed(42)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9D8xt6ymIOu",
        "outputId": "77561aa6-83bd-4e62-d915-9fc136806efb"
      },
      "source": [
        "# step-0 prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# convert single row with multiple columns to multiple rows single colums\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "\n",
        "# step-1 model setup\n",
        "\n",
        "# logistic regression, f = w*x+b and apply sigmoid at the end\n",
        "class MyLogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(MyLogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(in_features=n_input_features, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "\n",
        "model = MyLogisticRegression(n_features)\n",
        "\n",
        "# initialize model parameters manually\n",
        "for param in model.parameters():\n",
        "    param.data.fill_(0)\n",
        "\n",
        "# step-2 loss & optimizer\n",
        "learning_rate = 0.01\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# step-3 training loop\n",
        "n_iters = 100\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  # loss \n",
        "  J = loss(y_pred, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  J.backward()\n",
        "\n",
        "  # weight updates\n",
        "  optimizer.step()\n",
        "  # clear grads\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch {epoch+1}: loss = {J.item():.4f}')\n",
        "    # for name, param in model.named_parameters():\n",
        "    #   print(name, '-', param)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test)\n",
        "  cls_pred = y_pred.round() # if y_pred>=0.5 class = 1, else class = 0\n",
        "  test_acc = cls_pred.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'\\ntest accuracy = {test_acc:.3f}')"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10: loss = 0.5564\n",
            "epoch 20: loss = 0.4664\n",
            "epoch 30: loss = 0.4086\n",
            "epoch 40: loss = 0.3680\n",
            "epoch 50: loss = 0.3378\n",
            "epoch 60: loss = 0.3143\n",
            "epoch 70: loss = 0.2954\n",
            "epoch 80: loss = 0.2798\n",
            "epoch 90: loss = 0.2666\n",
            "epoch 100: loss = 0.2553\n",
            "\n",
            "test accuracy = 60.193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hkn9nXMH2j"
      },
      "source": [
        "# Tutorial 9 Dataset and DataLoader - Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL9hwn7_MQDe"
      },
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "bLvhesmrikgb",
        "outputId": "c055ccc0-f573-43e1-b4c1-0b16f37669d3"
      },
      "source": [
        "'''\n",
        "epoch = one forward and backward pass of ALL training samples\n",
        "batch_size = number of training samples used in one forward/backward pass\n",
        "number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\n",
        "e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
        "'''"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nepoch = one forward and backward pass of ALL training samples\\nbatch_size = number of training samples used in one forward/backward pass\\nnumber of iterations = number of passes, each pass (forward+backward) using [batch_size] number of samples\\ne.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaP-PD7FesRr",
        "outputId": "3b95bafa-12a9-4b77-c795-221f77a0e3c0"
      },
      "source": [
        "# mount gdrive with this code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbUxpCm9Z-Ca"
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = torch.from_numpy(wine_ds[:, 1:]) # take all rows akip first column\n",
        "    self.y = torch.from_numpy(wine_ds[:, [0]]) # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "    "
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8XkFSKyaMVP",
        "outputId": "273a02be-f3e9-4f26-b893-dd07e4e690db"
      },
      "source": [
        "wine_ds = WineDataset()\n",
        "wine_ds[0]"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
              "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
              "         1.0650e+03]), tensor([1.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98KixUYIhPyf",
        "outputId": "7acb5360-1df4-4720-8ea7-8bfaeca094fe"
      },
      "source": [
        "dataloader = DataLoader(dataset=wine_ds, batch_size=4, shuffle=True, num_workers=2)\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "data = data_iter.next()\n",
        "features, labels = data\n",
        "print(features, labels)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.2670e+01, 9.8000e-01, 2.2400e+00, 1.8000e+01, 9.9000e+01, 2.2000e+00,\n",
            "         1.9400e+00, 3.0000e-01, 1.4600e+00, 2.6200e+00, 1.2300e+00, 3.1600e+00,\n",
            "         4.5000e+02],\n",
            "        [1.3160e+01, 2.3600e+00, 2.6700e+00, 1.8600e+01, 1.0100e+02, 2.8000e+00,\n",
            "         3.2400e+00, 3.0000e-01, 2.8100e+00, 5.6800e+00, 1.0300e+00, 3.1700e+00,\n",
            "         1.1850e+03],\n",
            "        [1.2080e+01, 1.3900e+00, 2.5000e+00, 2.2500e+01, 8.4000e+01, 2.5600e+00,\n",
            "         2.2900e+00, 4.3000e-01, 1.0400e+00, 2.9000e+00, 9.3000e-01, 3.1900e+00,\n",
            "         3.8500e+02],\n",
            "        [1.2990e+01, 1.6700e+00, 2.6000e+00, 3.0000e+01, 1.3900e+02, 3.3000e+00,\n",
            "         2.8900e+00, 2.1000e-01, 1.9600e+00, 3.3500e+00, 1.3100e+00, 3.5000e+00,\n",
            "         9.8500e+02]]) tensor([[2.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTX8Ptpki0XP",
        "outputId": "fbeea1e7-a8b3-4fb2-c655-4b712f3522e1"
      },
      "source": [
        "# iterate over dataset by batched (dummy train loop)\n",
        "num_epochs = 2\n",
        "total_sample = len(wine_ds)\n",
        "num_iterations = math.ceil(total_sample/4)\n",
        "print('total samples =', total_sample, '\\n# of iterations(per epoch) =', num_iterations, '\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for step, (inputs, labels) in enumerate(dataloader):\n",
        "    if (step+1)%5==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step {step+1}/{num_iterations}, input_dim - {inputs.shape}')"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total samples = 178 \n",
            "# of iterations(per epoch) = 45 \n",
            "\n",
            "epoch 1/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 1/2, step 45/45, input_dim - torch.Size([2, 13])\n",
            "epoch 2/2, step 5/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 10/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 15/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 20/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 25/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 30/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 35/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 40/45, input_dim - torch.Size([4, 13])\n",
            "epoch 2/2, step 45/45, input_dim - torch.Size([2, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aky_2rAm7E8"
      },
      "source": [
        "# Tutorial 10 Dataset Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9o9kYUPv492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f6bd43df-301c-44d0-88b9-8ab07e9cc3cd"
      },
      "source": [
        "'''\n",
        "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
        "during creation of the DataSet\n",
        "\n",
        "complete list of built-in transforms: \n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "On Images\n",
        "---------\n",
        "CenterCrop, Grayscale, Pad, RandomAffine\n",
        "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
        "Resize, Scale\n",
        "\n",
        "On Tensors\n",
        "----------\n",
        "LinearTransformation, Normalize, RandomErasing\n",
        "\n",
        "Conversion\n",
        "----------\n",
        "ToPILImage: from tensor or ndrarray\n",
        "ToTensor : from numpy.ndarray or PILImage\n",
        "\n",
        "Generic\n",
        "-------\n",
        "Use Lambda \n",
        "\n",
        "Custom\n",
        "------\n",
        "Write own class\n",
        "\n",
        "Compose multiple Transforms\n",
        "---------------------------\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "'''"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTransforms can be applied to PIL images, tensors, ndarrays, or custom data\\nduring creation of the DataSet\\n\\ncomplete list of built-in transforms: \\nhttps://pytorch.org/docs/stable/torchvision/transforms.html\\n\\nOn Images\\n---------\\nCenterCrop, Grayscale, Pad, RandomAffine\\nRandomCrop, RandomHorizontalFlip, RandomRotation\\nResize, Scale\\n\\nOn Tensors\\n----------\\nLinearTransformation, Normalize, RandomErasing\\n\\nConversion\\n----------\\nToPILImage: from tensor or ndrarray\\nToTensor : from numpy.ndarray or PILImage\\n\\nGeneric\\n-------\\nUse Lambda \\n\\nCustom\\n------\\nWrite own class\\n\\nCompose multiple Transforms\\n---------------------------\\ncomposed = transforms.Compose([Rescale(256),\\n                               RandomCrop(224)])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elw0nDLBkMze"
      },
      "source": [
        "# dataset class with transform\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, transform=None):\n",
        "    # data loading\n",
        "    wine_ds = np.loadtxt('/content/drive/My Drive/Colab Notebooks/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "    self.x = wine_ds[:, 1:] # take all rows akip first column\n",
        "    self.y = wine_ds[:, [0]] # take all rows first column\n",
        "    self.n_samples = wine_ds.shape[0]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEH8dcyHsUfA"
      },
      "source": [
        "# custom transform classes (transform using callable objects)\n",
        "\n",
        "class MyToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MyMulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs*=self.factor\n",
        "    return inputs, target"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RhxQ2ltFKv",
        "outputId": "a29b7f8c-f3a7-49f1-f727-9f6862c9a796"
      },
      "source": [
        "dataset = WineDataset()\n",
        "features, labels = dataset[0]\n",
        "print('without transform:', type(features), type(labels))\n",
        "print('without transform: features =', features)\n",
        "\n",
        "dataset = WineDataset(transform=MyToTensor())\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyToTensor() transform:', type(features), type(labels))\n",
        "\n",
        "dataset = WineDataset(transform=MyMulTransform(2))\n",
        "features, labels = dataset[0]\n",
        "print('\\nwith MyMulTransform(2) transform: features =', features)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "without transform: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "without transform: features = [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
            "\n",
            "with MyToTensor() transform: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "\n",
            "with MyMulTransform(2) transform: features = [2.846e+01 3.420e+00 4.860e+00 3.120e+01 2.540e+02 5.600e+00 6.120e+00\n",
            " 5.600e-01 4.580e+00 1.128e+01 2.080e+00 7.840e+00 2.130e+03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enO2D5rgjxsX"
      },
      "source": [
        "# Tutorial 11 Softmax and Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYUN0W4WkBrk",
        "outputId": "7aae8da5-a73e-474d-beb8-2d13214e16df"
      },
      "source": [
        "# softmax using numpy\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0, 1.0, 0.1])\n",
        "sm_x = softmax(x)\n",
        "print(sm_x)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.65900114 0.24243297 0.09856589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2Mi4fXndhw",
        "outputId": "2e839892-030a-42da-ef99-1275c9f022c6"
      },
      "source": [
        "# softmax using pytorch\n",
        "\n",
        "x = torch.tensor([2.0, 1.0, 0.1])\n",
        "sm_x = torch.softmax(x, dim=0)\n",
        "print(sm_x)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Q7XPRJtdJ7",
        "outputId": "f52898b5-da74-4432-dfba-5fb3a49c9ada"
      },
      "source": [
        "# cross entropy using numpy\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "  loss = -np.sum(actual * np.log(predicted))\n",
        "  return loss # / float(predicted.shape[0])\n",
        "\n",
        "y_actual = np.array([1, 0, 0]) # one hot encoded\n",
        "\n",
        "y_good_pred = np.array([0.7, 0.2, 0.1])\n",
        "y_bad_pred = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(y_actual, y_good_pred)\n",
        "l2 = cross_entropy(y_actual, y_bad_pred)\n",
        "print(f'loss for good prediction={l1:.3f}')\n",
        "print(f'loss for bad prediction={l2:.3f}')"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss for good prediction=0.357\n",
            "loss for bad prediction=2.303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJT7jjhfvMJs",
        "outputId": "78310fac-af7f-46b2-e28c-c40008cd994a"
      },
      "source": [
        "# cross entropy using pytorch (with multiple samples)\n",
        "\n",
        "'''\n",
        "nn.CrossEntropyLoss() automatically applies, \n",
        "  nn.LogSoftmax + nn.NLLLoss(neg log likelihood loss)\n",
        "\n",
        "- Don't apply softmax in the last layer yourself\n",
        "- y_actual has class labels, not One Hot Encoded!\n",
        "- y_pred has raw scores and not prob values after softmax\n",
        "'''\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "y_actual = torch.tensor([2, 0, 1]) # 3 samples\n",
        "\n",
        "# 3 sample each sample can be one of 3 classes\n",
        "y_pred_good = torch.tensor([ [0.2, 1.0, 2.0],[2.0, 1.0, 0.1],[1.0, 2.0, 0.1] ])\n",
        "y_pred_bad = torch.tensor([ [0.5, 2.0, 0.3],[0.5, 1.0, 2.3],[7.5, 2.0, 1.3] ]) \n",
        "\n",
        "l1 = loss(y_pred_good, y_actual)\n",
        "l2 = loss(y_pred_bad, y_actual)\n",
        "print('good pred loss =', l1.item())\n",
        "print('bad pred loss =', l2.item())\n",
        "\n",
        "_, class_pred_good = torch.max(y_pred_good, 1)\n",
        "_, class_pred_bad = torch.max(y_pred_bad, 1)\n",
        "print('\\ngood class prediction =', class_pred_good)\n",
        "print('bad class prediction =', class_pred_bad)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good pred loss = 0.4204676151275635\n",
            "bad pred loss = 3.236616373062134\n",
            "\n",
            "good class prediction = tensor([2, 0, 1])\n",
            "bad class prediction = tensor([1, 2, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xEe5i8E80rb"
      },
      "source": [
        "### Implementing Basic Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqfaCRFc0TVs"
      },
      "source": [
        "# custom multi-class neural network with one hidden layer and relu activation\n",
        "class MultiClassNeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(MultiClassNeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # input layer to hidden layer\n",
        "    output = self.relu(self.linear(input))\n",
        "    # hidden layer to output layer\n",
        "    output = self.linear2(output)\n",
        "    \n",
        "    # don't do softmax because nn.CrossEntropy does it for us\n",
        "\n",
        "    return output\n",
        "\n",
        "model = MultiClassNeuralNet(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "loss = nn.CrossEntropyLoss() # applies softmax to the output automatically"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGGOyRZQ_Dbm"
      },
      "source": [
        "# custom binary neural network with one hidden layer and relu activation\n",
        "class BinaryClassNeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(BinaryClassNeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # input layer to hidden layer\n",
        "    output = self.relu(self.linear1(input))\n",
        "    # hidden layer to output layer\n",
        "    output = self.linear2(output)\n",
        "    # probability prediction\n",
        "    y_pred = torch.sigmoid(output)\n",
        "    return y_pred\n",
        "\n",
        "model = BinaryClassNeuralNet(input_size=28*28, hidden_size=5)\n",
        "loss = nn.BCELoss()"
      ],
      "execution_count": 253,
      "outputs": []
    }
  ]
}