{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVJc3JzbnZXnKaAXBicmV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferdouszislam/pytorch-practice/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn # required at Tutorial 06"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R12lgukzbI5h",
        "outputId": "546909e7-0c8f-4ff7-d0f2-85252ffde8d3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('GPU, yay!')\n",
        "else:\n",
        "  print('CPU :(')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU :(\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgEIW4ARGzV"
      },
      "source": [
        "# Tutorial 02 - Tensor Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chx2s2PmOo7Y"
      },
      "source": [
        "x = torch.rand(3,3)\n",
        "y = torch.rand(3,3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3mSyMQPXIC",
        "outputId": "a61431a9-ec66-46b3-b41c-17c628c8f631"
      },
      "source": [
        "print(x, '\\n', y, '\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6481, 0.4903, 0.0836],\n",
            "        [0.2646, 0.7901, 0.0386],\n",
            "        [0.0805, 0.3598, 0.3267]]) \n",
            " tensor([[0.4114, 0.5518, 0.9380],\n",
            "        [0.1188, 0.1703, 0.4574],\n",
            "        [0.9259, 0.1294, 0.2896]]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSUrPReMQ2rY",
        "outputId": "f6944054-41e4-4728-df4c-0deac77e0f1d"
      },
      "source": [
        "z = torch.add(x, y)\n",
        "z"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0594, 1.0421, 1.0216],\n",
              "        [0.3834, 0.9604, 0.4959],\n",
              "        [1.0064, 0.4892, 0.6163]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uhX6x6QQsj",
        "outputId": "ad3ff0de-4d39-4eac-8f06-d82d2abf4ab2"
      },
      "source": [
        "z = torch.mul(x, y)\n",
        "z "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2666, 0.2705, 0.0784],\n",
              "        [0.0314, 0.1345, 0.0176],\n",
              "        [0.0745, 0.0466, 0.0946]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oSZ4XYCY4Dp",
        "outputId": "bac0464c-1a12-4644-d76d-c3d959f662f6"
      },
      "source": [
        "z.add_(y) # same as z+=y"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6779, 0.8223, 1.0164],\n",
              "        [0.1503, 0.3048, 0.4750],\n",
              "        [1.0004, 0.1760, 0.3842]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zHW88RaR5ra",
        "outputId": "3eaf8a5c-4f76-4ce4-e9b1-80e9ce89b61b"
      },
      "source": [
        "z[:, 2] # get all rows at column 2 (0 based indexing)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0164, 0.4750, 0.3842])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNIPYZ-SJlD",
        "outputId": "9b1ecd98-ee4e-4054-97a7-c8fb109785ee"
      },
      "source": [
        "z[1,2].item() # get single element value"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4749874770641327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5EidfvTBAQ"
      },
      "source": [
        "'''\n",
        "input- a pytorch tensor variable \n",
        "returns- multiplication of the input tensor's dimensions \n",
        "'''\n",
        "def get_flat_shape(tensor):\n",
        "  dims=list(tensor.size())\n",
        "  flat_dim = 1\n",
        "  for dim in dims:\n",
        "    flat_dim*=dim\n",
        "  return flat_dim"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7x_Wm3kUBEd",
        "outputId": "b278440f-1ca1-40d1-e543-71700c357573"
      },
      "source": [
        "get_flat_shape(z)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Wk3k4YSW3G",
        "outputId": "da9f6455-48d3-4615-eb8a-07948d80345d"
      },
      "source": [
        "flat_z = z.view(get_flat_shape(z)) # resizing a tensor\n",
        "flat_z"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6779, 0.8223, 1.0164, 0.1503, 0.3048, 0.4750, 1.0004, 0.1760, 0.3842])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fB2PMBHWRHJ",
        "outputId": "f14212a7-4477-4702-a7f9-2f6c35858571"
      },
      "source": [
        "# tensor to numpy array conversion\n",
        "np_z = flat_z.clone().numpy() # using '.clone()' is a MUST\n",
        "np_z"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.67794675, 0.8223072 , 1.0164254 , 0.15028793, 0.304846  ,\n",
              "       0.47498748, 1.000425  , 0.17596233, 0.3842081 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1MtPnKZyi5",
        "outputId": "f296dcbf-7903-4911-f534-e5065bf28c5f"
      },
      "source": [
        "# numpy array to tensor conversion\n",
        "flat_z = torch.from_numpy(np_z.copy()) # using '.copy()' is a MUST\n",
        "flat_z"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6779, 0.8223, 1.0164, 0.1503, 0.3048, 0.4750, 1.0004, 0.1760, 0.3842])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CC9XZmKcP-_"
      },
      "source": [
        "  \n",
        "  **Tensors can be kept into GPU but numpy arrays have to remain on CPU. GPUs are generally faster.**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uD1SwRcgmt"
      },
      "source": [
        "device = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMmS2CFUdBMh"
      },
      "source": [
        "# all operations on tensors to be done in GPU\n",
        "# x = torch.rand(2, 2).to(device)\n",
        "# y = torch.rand(2, 2).to(device)\n",
        "# z=x+y\n",
        "\n",
        "# print(x, '\\n', y, '\\n', z)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFi3id9d7Kl",
        "outputId": "b834fb08-daff-43bc-ab7d-8fb2e9b26755"
      },
      "source": [
        "# numpy arrays MUST be on cpu\n",
        "z = z.to('cpu')\n",
        "np_z = z.clone().numpy()\n",
        "np_z"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.67794675, 0.8223072 , 1.0164254 ],\n",
              "       [0.15028793, 0.304846  , 0.47498748],\n",
              "       [1.000425  , 0.17596233, 0.3842081 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUVKnRYzensC"
      },
      "source": [
        "# Tutorial 03 - Gradient Calculation with Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZDbHzXeuvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f65a6b-56c5-41bd-d7d2-1d67565aaf00"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "x"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0305, -0.9509, -0.4911], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c1iYKHiVRm",
        "outputId": "db295e20-b380-4367-e79d-ede5eb736526"
      },
      "source": [
        "y=x+2\n",
        "print(y)\n",
        "z=y*y*2\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0305, 1.0491, 1.5089], grad_fn=<AddBackward0>)\n",
            "tensor([8.2458, 2.2012, 4.5533], grad_fn=<MulBackward0>)\n",
            "tensor(5.0001, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIr-7JvAnX7k",
        "outputId": "0373e7e9-546c-4351-e05b-4cde358f47dd"
      },
      "source": [
        "# calculating dz/dx for each element of x tensor (in this case- x1,x2,x3)\n",
        "#  N.B- All but the last call to backward should have the retain_graph=True option\n",
        "z.backward(retain_graph=True)\n",
        "print(x.grad)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.7073, 1.3988, 2.0118])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4mCj276LGEC",
        "outputId": "db47c028-e6a7-40a4-c661-0bc237b236ec"
      },
      "source": [
        "# prevent gradient tracking \n",
        "# (might be needed when updating weights during training)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# way 1\n",
        "print('way 1')\n",
        "y=x\n",
        "y.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "# way 2\n",
        "print('way 2')\n",
        "y=x.detach()\n",
        "print(y)\n",
        "\n",
        "# way 3\n",
        "print('way 3')\n",
        "y=x+2\n",
        "print(x, '\\n', y)\n",
        "with torch.no_grad():\n",
        "  y=x+2\n",
        "  print(x, '\\n', y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0969, -1.2457, -0.5860], requires_grad=True)\n",
            "way 1\n",
            "tensor([ 0.0969, -1.2457, -0.5860])\n",
            "way 2\n",
            "tensor([ 0.0969, -1.2457, -0.5860])\n",
            "way 3\n",
            "tensor([ 0.0969, -1.2457, -0.5860]) \n",
            " tensor([2.0969, 0.7543, 1.4140])\n",
            "tensor([ 0.0969, -1.2457, -0.5860]) \n",
            " tensor([2.0969, 0.7543, 1.4140])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsZzzjcWMlyQ",
        "outputId": "76fe9655-1c25-440e-bd54-690a4ad838b9"
      },
      "source": [
        "# dummy training example with some weights\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum() # loss function... probably\n",
        "  \n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  # before next iteration or optimization step MUST empty the gradient\n",
        "  weights.grad.zero_()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JJUSOKcaArS"
      },
      "source": [
        "# Tutorial 04 - Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI7Rq6laH1F",
        "outputId": "ed74cf53-8e9a-464b-8037-508dbfdd2e10"
      },
      "source": [
        "# example backpropagation for a single instance\n",
        "\n",
        "x = torch.tensor(1.0) # input\n",
        "y = torch.tensor(2.0) # actual output\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # weight i.e, learnable parameter\n",
        "\n",
        "# forward pass\n",
        "y_hat = w*x # y_hat is the prediction using linear model = w*x \n",
        "loss = (y_hat - y)**2 # loss function = squared error (generally this would be MSE)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# now update weight using the gradient \n",
        "# and do forward and backward pass again"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWuYsc7Pkz5"
      },
      "source": [
        "# Tutorial 05 - Gradient Descent with Autograd & Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrrkb_gP1Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98baeb97-b460-4887-8e84-58d930ee55a6"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = np.array([1,2,3,4], dtype=np.float32) # input\n",
        "Y = np.array([2,4,6,8], dtype=np.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 5\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction, forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # gradient\n",
        "  dJ_dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - learning_rate* dJ_dw \n",
        "\n",
        "  # print everytime\n",
        "  if epoch%1==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOYrO-OdpBP"
      },
      "source": [
        "### Now let's do the same using Autograd for backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJWLzMGadnK1",
        "outputId": "30346912-fa4c-4a77-cfb8-a571b8108632"
      },
      "source": [
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32) # input\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32) # actual output\n",
        "\n",
        "# randomely initializing weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss function, MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# here, loss, J = 1/N * (w*x-y)^2 [because y_pred = w*x]\n",
        "# therefore, dJ/dw = 1/N*2*x*(w*x-y)\n",
        "# def gradient(x, y, y_pred):\n",
        "#   return np.dot(2*x, y_pred-y).mean()\n",
        "\n",
        "print(f'Prediction before training for x=5 : {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  #dJ_dw = gradient(X, Y, y_pred)\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights\n",
        "  #w = w - learning_rate* dJ_dw \n",
        "  w.data = w.data - learning_rate * w.grad\n",
        "  # alternately we can do this,\n",
        "  # with torch.no_grad(): \n",
        "  #   # update to weight should not be tracked for calculating gradient\n",
        "  #   w -= learning_rate*w.grad\n",
        "\n",
        "  w.grad.zero_() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {J:.8f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {forward(5):.3f}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction before training for x=5 : 0.000\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OsGme4_5u_"
      },
      "source": [
        "# Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmGAvNn0C4pW"
      },
      "source": [
        "### Implementing Linear Regression same as before but this time with model, loss, optimizer, autograd from the **torch.nn** library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNuXnTD_4c-",
        "outputId": "3f94b6ce-96b8-4392-8fce-8bf36eab1402"
      },
      "source": [
        "'''\n",
        "Training Pipeline\n",
        "\n",
        "1. design model (input size, output size, forward pass)\n",
        "2. Construct loss & optimizer\n",
        "3. Training loop\n",
        "  - forward pass: compute prediction\n",
        "  - backward pass: compute gradients\n",
        "  - update weights\n",
        "'''\n",
        "# implementing linear regression from scratch with dummy data\n",
        "\n",
        "# f = w*x, for the example model below- w = 2 fits the ouput completely \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) # input\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32) # actual output\n",
        "\n",
        "n_samples, n_features = X.shape # 4 samples each with one feature\n",
        "print('# of samples =', n_samples, ' # of features =', n_features, '\\n')\n",
        "\n",
        "# define custom model (same as Linear Regression for now)\n",
        "class MyLinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "# model = nn.Linear(in_features=n_features, out_features=1)\n",
        "model = MyLinearRegression(input_dim=n_features, output_dim=1)\n",
        "\n",
        "# test data\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "print(f'Prediction before training for x=5 : {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20 \n",
        "# MSE as loss function\n",
        "loss = nn.MSELoss()\n",
        "# optimize model with stochastic gradient descent\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print('\\n[Training started...]\\n')\n",
        "for epoch in range(n_iters):\n",
        "  # prediction i.e forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  J = loss(Y, y_pred)\n",
        "\n",
        "  # calculate gradient i.e backward pass\n",
        "  J.backward() # dJ/dw\n",
        "\n",
        "  # update weights using optimizer\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad() # clear the gradients \n",
        "\n",
        "  if epoch%2==1:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {J:.3f}')\n",
        "print('\\n[Training finished...]\\n')\n",
        "\n",
        "print(f'Prediction after training for x=5 : {model(X_test).item():.3f}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of samples = 4  # of features = 1 \n",
            "\n",
            "Prediction before training for x=5 : 3.973\n",
            "\n",
            "[Training started...]\n",
            "\n",
            "epoch 2: w = 1.272, loss = 8.679\n",
            "epoch 4: w = 1.518, loss = 4.183\n",
            "epoch 6: w = 1.688, loss = 2.018\n",
            "epoch 8: w = 1.806, loss = 0.976\n",
            "epoch 10: w = 1.888, loss = 0.474\n",
            "epoch 12: w = 1.944, loss = 0.232\n",
            "epoch 14: w = 1.983, loss = 0.116\n",
            "epoch 16: w = 2.010, loss = 0.060\n",
            "epoch 18: w = 2.029, loss = 0.033\n",
            "epoch 20: w = 2.042, loss = 0.020\n",
            "\n",
            "[Training finished...]\n",
            "\n",
            "Prediction after training for x=5 : 9.988\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}